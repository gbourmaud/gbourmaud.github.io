{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP Auto-encodeur variationnel\n",
    "Dans ce TP, vous allez implémenter un auto-encodeur variationnel (VAE). Afin de pouvoir visualiser les différentes densités de probabilité présentes dans la théorie d'un VAE, nous allons considérer un problème en 1D. Le fait de considérer un exemple 1D nous permettra également d'effectuer des apprentissages sur CPU en quelques secondes. L'implémentation se fera en PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "#### 4) Configurer Spyder en suivant ces instructions [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créez un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch as t\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Génération des données en 1D\n",
    "L'objectif du VAE que nous implémentons est d'apprendre à générer de nouveaux échantillons qui ressemblent aux échantillons de la base de données $\\{x_i\\}_{i=1...N}$. Dans un cas réel, ces échantillons sont supposés issus d'une densité de probabilité inconnue $p_{data}(x)$. Dans notre contexte, nous simulons ces données $\\{x_i\\}_{i=1...N}$ ainsi nous avons besoin de définir $p_{data}(x)$. Nous considérerons un mélange de gaussiennes : $p_{data}(x)=\\sum_{j=1...C} w_j \\mathcal{N}(x;\\mu_j,\\sigma_j)$. Définissons les paramètres de ce mélange :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = t.tensor([0.2, 0.45, 0.35, 0.1])\n",
    "w_list /=w_list.sum() \n",
    "\n",
    "mu_list = t.tensor([-3., 2.5, 1.5, -2.5])\n",
    "sigma_list = t.tensor([0.3, 1.2, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le but d'afficher $p_{data}(x)$, définissons la fonction `plot_pdata` et effectuons un affichage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdata(w_list, mu_list, sigma_list, ax=0, orientation='vertical'):\n",
    "    \n",
    "    x = t.linspace(start=-6., end=8., steps=1000)\n",
    "    \n",
    "    p_data = t.zeros_like(x)\n",
    "    for i in range(w_list.shape[0]):\n",
    "        p_data_i = (1./(sigma_list[i]*math.sqrt(2*math.pi))*t.exp(-0.5*((x-mu_list[i])/sigma_list[i])**2))\n",
    "        p_data += w_list[i]*p_data_i\n",
    "        \n",
    "    if(ax==0):\n",
    "        if(orientation=='vertical'):\n",
    "            plt.plot(x,p_data,'k')\n",
    "        else:\n",
    "            plt.plot(p_data,x,'k')\n",
    "    else:\n",
    "        if(orientation=='vertical'):\n",
    "            ax.plot(x,p_data,'k')\n",
    "        else:\n",
    "            ax.plot(p_data,x,'k')\n",
    "    return\n",
    "\n",
    "plt.figure(1)\n",
    "plot_pdata(w_list, mu_list, sigma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais échantillonner notre base de données $\\{x_i\\}_{i=1...N}$ et afficher son histogramme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_pdata(N, w_list, mu_list, sigma_list):\n",
    "    \n",
    "    n_c = w_list.shape[0]\n",
    "\n",
    "    samp = t.zeros((N,1))\n",
    "    mask = t.multinomial(w_list,num_samples=N,replacement=True)\n",
    "    \n",
    "    for i in range(n_c):\n",
    "        samp_i = t.normal(mean=mu_list[i], std=sigma_list[i], size=(N,1))\n",
    "        samp[mask==i] = samp_i[mask==i]\n",
    " \n",
    "    return samp\n",
    "\n",
    "N_samp = int(2e5)\n",
    "X = sample_from_pdata(N_samp, w_list, mu_list, sigma_list)\n",
    "nbins = 100\n",
    "n, bins, patches = plt.hist(X.numpy(), nbins, density=True, facecolor='r', alpha=0.75)\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de l'encodeur\n",
    "Afin de pouvoir afficher les densités de probabilité, nous considérerons un espace latent de dimension 1. Ainsi l'encodeur est un réseau de neurones dont l'entrée est un scalaire $x$ et dont les sorties sont deux scalaires, les paramètres de $q_m (z|x,\\phi)=\\mathcal{N}(x;\\mu_{z|x},\\sigma_{z|x}^2)$. Pour des raisons de stabilité numérique qui deviendront claires par la suite, au lieu de prédire l'écart-type $\\sigma_{z|x}$, le réseau prédira le logarithme de la variance $\\alpha_{z|x}=\\ln (\\sigma_{z|x}^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encodeur(nn.Module):\n",
    "    def __init__(self,H,zDim):\n",
    "        super(encodeur, self).__init__()\n",
    "        self.H = H\n",
    "        self.zDim = zDim\n",
    "        self.linearIn = nn.Linear(1, H)\n",
    "        self.activIn = nn.Tanh()\n",
    "        \n",
    "        self.linearHidden = nn.Linear(H, H)\n",
    "        self.activHidden = nn.Tanh()\n",
    "        \n",
    "        self.linearOut = nn.Linear(H, 2*zDim)\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.linearIn(x)\n",
    "        out = self.activIn(out)\n",
    "        \n",
    "        out = self.linearHidden(out)\n",
    "        out = self.activHidden(out)\n",
    "        \n",
    "        out = self.linearOut(out)\n",
    "\n",
    "        mu_z = out[:,:self.zDim]\n",
    "        logvar_z = out[:,self.zDim:]\n",
    "        \n",
    "        return mu_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail : Dessiner le schéma de l'encodeur sur une feuille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du décodeur\n",
    "Le décodeur est un réseau de neurones dont l'entrée est un scalaire $z$ et dont la sortie est également un scalaire, à savoir la moyenne de $p_m (x|z,\\theta)=\\mathcal{N}(z;\\mu_{x|z},\\sigma_{x|z}^2)$. Dans notre cas, l'écart-type $\\sigma_{x|z}$ n'est pas une sortie du décodeur mais un hyperparamètre qu'il va falloir régler manuellement. Nous reviendrons sur ce point par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decodeur(nn.Module):\n",
    "    def __init__(self,H,zDim):\n",
    "        super(decodeur, self).__init__()\n",
    "        \n",
    "        self.H = H\n",
    "        self.zDim = zDim\n",
    "        \n",
    "        self.linearIn = nn.Linear(zDim, H)\n",
    "        self.activIn = nn.Tanh()\n",
    "        \n",
    "        self.linearHidden = nn.Linear(H, H)\n",
    "        self.activHidden = nn.Tanh()\n",
    "        \n",
    "        self.linearOut = nn.Linear(H, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.linearIn(x)\n",
    "        out = self.activIn(out)\n",
    "        \n",
    "        out = self.linearHidden(out)\n",
    "        out = self.activHidden(out)\n",
    "        \n",
    "        mu_x = self.linearOut(out)\n",
    "\n",
    "        return mu_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail : Dessiner le schéma du décodeur sur une feuille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 100\n",
    "zDim = 1\n",
    "learning_rate = 1e-3\n",
    "batchSize = 256\n",
    "inverseVarianceLikelihood = 5e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation de l'encodeur, du décodeur et de l'optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encodeur(H,zDim)\n",
    "dec = decodeur(H,zDim)\n",
    "optimizer = t.optim.Adam(list(enc.parameters()) + list(dec.parameters()), lr=learning_rate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage\n",
    "Nous pouvons créer une boucle qui va implémenter les itérations de la descente de gradient stochastique (ainsi qu'une figure que nous utiliserons pour afficher l'évolution de l'apprentissage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NItMax = 40000\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "for i in range(NItMax):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tirons un minibatch et encodons-le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = t.randperm(N_samp)\n",
    "X_batch = X[perm[:batchSize],:]\n",
    "mu_z, logvar_z = enc.forward(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut maintenant tirer des échantillons de $p_m (x|z,\\theta)=\\mathcal{N}(z;\\mu_{x|z},\\sigma_{x|z}^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_samp = t.normal(mean=0.,std=1.,size=logvar_z.shape)\n",
    "z_samp = mu_z + (logvar_z*0.5).exp_()*eps_samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décodons ces échantillons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_x = dec.forward(z_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du VAE est de minimiser la divergence de Kullback-Leibler $KL(p_{data}(x)q_{m}(z|x, \\phi)||p(z)p_{m}(x|z, \\theta ))$ par rapport à $\\phi$ et $\\theta$. Après simplification, cette fonction de coût devient : $\\mathcal{L}(\\theta,\\phi) = KL(\\mathcal{N}(z;\\mu_{z|x},\\sigma_{z|x}^2)||\\mathcal{N}(z;0,1)) -\\ln(\\mathcal{N}(x;\\mu_{x|z},\\sigma_{x|z}^2))+\\text{cst}_{\\phi,\\theta}$.\n",
    "Le premier terme est la divergence de Kullback-Leibler entre deux gaussiennes qui a pour expression $\\frac{1}{2} (\\sigma_{z|x}^2 + \\mu_{z|x}^2 -1 -\\ln(\\sigma_{z|x}^2))$. Nous voyons que le dernier terme est le logarithme de la variance ce qui peut poser des problèmes de stabilité numérique. Ce terme explique la paramétrisation choisie pour la sortie de l'encodeur $\\alpha_{z|x}=\\ln (\\sigma_{z|x}^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLD = 0.5*(logvar_z.exp() - logvar_z - 1 + (mu_z**2)).sum()\n",
    "KLD /= logvar_z.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le second terme est l'opposé du logarithme d'une gaussienne : $-\\ln(\\mathcal{N}(x;\\mu_{x|z},\\sigma_{x|z}^2)=\\frac{1}{2\\sigma_{x|z}^2}(x-\\mu_{x|z})^2 + \\text{cst}$. Ici le coefficient $\\frac{1}{\\sigma_{x|z}^2}$ correspond à l'hyperparamètre `inverseVarianceLikelihood` défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = (0.5*inverseVarianceLikelihood*(X_batch-mu_x)**2).sum()\n",
    "likelihood /= logvar_z.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = KLD+likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminons une itération de descente de gradient stochastique en calculant les gradients et en mettant à jour les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "l.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "D'après l'équation de la fonction de coût, quel comportement aura le VAE si l'hyperparamètre `inverseVarianceLikelihood` (coefficient $\\frac{1}{\\sigma_{x|z}^2}$) vaut $0$ ? Et si `inverseVarianceLikelihood` (coefficient $\\frac{1}{\\sigma_{x|z}^2}$) a une valeur très grande ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichages\n",
    "Afin de contrôler l'évolution de l'apprentissage nous pouvons afficher des informations dans le terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('It {} : loss : {:.2e}, KLD : {:.2e}, likelihood : {:.2e}, lr : {}'.format(i, l.item(), KLD.item(), likelihood.item(), optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et effectuer des affichages régulièrement dans une figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if((i<1000 and i%200==0) or i%2000 == 0):\n",
    "        with t.no_grad():\n",
    "            plt.clf()\n",
    "            mu_z_v, logvar_z_v = enc.forward(X)\n",
    "            eps_samp_v = t.normal(mean=0.,std=1.,size=logvar_z_v.shape)\n",
    "            z_samp_v = mu_z_v + (logvar_z_v*0.5).exp_()*eps_samp_v\n",
    "            x_samp_v = dec.forward(z_samp_v)\n",
    "            \n",
    "            new_Z_v = t.normal(mean=0.,std=1.,size=[10000,zDim])\n",
    "            new_mu_v = dec.forward(new_Z_v)\n",
    "            eps_samp_v = t.normal(mean=0.,std=1.,size=new_mu_v.shape)\n",
    "            new_X_v = new_mu_v + (1./(math.sqrt(inverseVarianceLikelihood)))*eps_samp_v\n",
    "            \n",
    "            plotHistograms(fig, w_list, mu_list, sigma_list, nbins, X, mu_z_v, z_samp_v, x_samp_v, new_Z_v, new_mu_v, new_X_v)\n",
    "            plt.pause(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code précédent fait appel à la fonction `plotHistograms` que voici (à mettre en haut du script) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistograms(fig, w_list, mu_list, sigma_list, nbins, X, mu_z, z_samp, x_samp, new_Z, new_mu_X, new_X):\n",
    "    left, width = 0.1, 0.65/3\n",
    "    bottom, height = 0.1, 0.65\n",
    "    \n",
    "    hist_height = 0.2/3\n",
    "    \n",
    "    spacing = 0.02\n",
    "    \n",
    "    rect_plt_train_enc = [left+hist_height, bottom+hist_height, width, height]\n",
    "    rect_histx_train_enc = [left+hist_height, bottom , width, hist_height]\n",
    "    rect_histy_train_enc = [left , bottom+hist_height, hist_height, height]\n",
    "    \n",
    "    rect_plt_train_dec = [width +hist_height+ spacing+left+hist_height, bottom+hist_height, width, height]\n",
    "    rect_histx_train_dec = [width + hist_height+spacing+left+hist_height, bottom , width, hist_height]\n",
    "    rect_histy_train_dec = [width + hist_height+spacing+left , bottom+hist_height, hist_height, height]\n",
    "    \n",
    "    rect_plt_train_gen = [2*(width +hist_height+ spacing)+left+hist_height, bottom+hist_height, width, height]\n",
    "    rect_histx_train_gen = [2*(width + hist_height+spacing)+left+hist_height, bottom , width, hist_height]\n",
    "    rect_histy_train_gen = [2*(width + hist_height+spacing)+left , bottom+hist_height, hist_height, height]\n",
    "        \n",
    "    #afficache encodeur données apprentissage\n",
    "    \n",
    "    ax = fig.add_axes(rect_plt_train_enc)\n",
    "    ax_histx = fig.add_axes(rect_histx_train_enc, sharex=ax)\n",
    "    ax_histx.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histx.invert_yaxis()\n",
    "    \n",
    "    ax_histy = fig.add_axes(rect_histy_train_enc, sharey=ax)\n",
    "    ax_histy.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histy.invert_xaxis()\n",
    "    \n",
    "    plot_pdata(w_list, mu_list, sigma_list,ax=ax_histx);\n",
    "    ax_histx.hist(X.numpy(), nbins, density=True, facecolor='r', alpha=0.75)\n",
    "    \n",
    "    plot_pdata(t.tensor([1.]), t.tensor([0.]), t.tensor([1.]), ax=ax_histy, orientation='horizontal');\n",
    "    ax_histy.hist(z_samp.numpy(), nbins, density=True, facecolor='g', alpha=0.75, orientation='horizontal')\n",
    "    \n",
    "    ax.scatter(X[::10],z_samp[::10], s=0.1 , label=r'échantillons de $p_{data}(x)q_{m}(z|x, \\phi )$')\n",
    "    \n",
    "    #ax.plot(X[::100], mu_z[::100], label='y=decodeur(x)')\n",
    "    ax.grid('on')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_title('Encodage données entraînement')\n",
    "    \n",
    "    #afficache décodeur données apprentissage\n",
    "    ax = fig.add_axes(rect_plt_train_dec)\n",
    "    ax_histx = fig.add_axes(rect_histx_train_dec, sharex=ax)\n",
    "    ax_histx.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histx.invert_yaxis()\n",
    "    \n",
    "    ax_histy = fig.add_axes(rect_histy_train_dec, sharey=ax)\n",
    "    ax_histy.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histy.invert_xaxis()\n",
    "    \n",
    "    plot_pdata(w_list, mu_list, sigma_list,ax=ax_histx, );\n",
    "    ax_histx.hist(x_samp.numpy(), nbins, density=True, facecolor='r', alpha=0.75)\n",
    "    \n",
    "    plot_pdata(t.tensor([1.]), t.tensor([0.]), t.tensor([1.]), ax=ax_histy, orientation='horizontal');\n",
    "    ax_histy.hist(z_samp.numpy(), nbins, density=True, facecolor='g', alpha=0.75, orientation='horizontal')\n",
    "    \n",
    "    x_samp_s,ind = t.sort(x_samp[::100],dim=0)\n",
    "    z_samp_temp = z_samp[::100]\n",
    "    z_samp_s = z_samp_temp[ind.view(-1)]\n",
    "    ax.plot(x_samp_s, z_samp_s, label='encodeur(y)=x')\n",
    "    ax.grid('on')\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_title('Décodage données entraînement')\n",
    "             \n",
    "    #afficache décodeur génération nouvelles données\n",
    "    ax = fig.add_axes(rect_plt_train_gen)\n",
    "    ax_histx = fig.add_axes(rect_histx_train_gen, sharex=ax)\n",
    "    ax_histx.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histx.invert_yaxis()\n",
    "    \n",
    "    ax_histy = fig.add_axes(rect_histy_train_gen, sharey=ax)\n",
    "    ax_histy.tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    ax_histy.invert_xaxis()\n",
    "\n",
    "    plot_pdata(w_list, mu_list, sigma_list,ax=ax_histx);\n",
    "    ax_histx.hist(new_X.numpy(), nbins, density=True, facecolor='b', alpha=0.75)\n",
    "    \n",
    "    plot_pdata(t.tensor([1.]), t.tensor([0.]), t.tensor([1.]), ax=ax_histy, orientation='horizontal');\n",
    "    ax_histy.hist(new_Z.numpy(), nbins, density=True, facecolor='c', alpha=0.75, orientation='horizontal')\n",
    "    \n",
    "    ax.scatter(new_X[::10],new_Z[::10], s=0.1 , label=r'échantillons de $p(z)p_{m}(x|z, \\theta )$')\n",
    "    #ax.plot(new_X[::10],new_Z[::10], label='encodeur(y)=x')\n",
    "    ax.grid('on')\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_title('Génération de nouvelles données')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez lancer un apprentissage et observer le comportement du VAE au cours des itérations. Sur la figure, pouvez-vous identifier l'impact de la valeur de l'hyperparamètre `inverseVarianceLikelihood` (coefficient $\\frac{1}{\\sigma_{x|z}^2}$) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez faire varier les valeurs des hyperparamètres, modifier les architectures de l'encodeur et du décodeur, ou encore changer la forme de $p_{data}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'il vous reste du temps, vous pouvez tenter d'implémenter un GAN pour résoudre le même problème."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
