{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP : Implémentation d'un réseau de neurones de type Perceptron multicouche pour un problème de classification\n",
    "Dans ce TP, vous allez implémenter en Numpy un réseau de neurones de type Perceptron multicouche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "#### 4) Configurer Spyder en suivant ces instructions [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Définition d'un exemple jouet à 3 classes en 2D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = np.array([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "y = np.array([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, H):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        #parameters\n",
    "        self.W1 = (np.sqrt(6./self.D))*(2*(np.random.uniform(size=(self.D,self.H))-0.5))\n",
    "        self.b1 = (1./np.sqrt(self.D))*(2*(np.random.uniform(size=(self.H))-0.5))\n",
    "        self.W3 = (np.sqrt(6./self.H))*(2*(np.random.uniform(size=(self.H,self.C))-0.5))\n",
    "        self.b3 = (1./np.sqrt(self.H))*(2*(np.random.uniform(size=(self.C))-0.5))\n",
    "        \n",
    "        \n",
    "        #gradients\n",
    "        self.dl_dW1 = np.zeros_like(self.W1)\n",
    "        self.dl_db1 = np.zeros_like(self.b1)\n",
    "        self.dl_dW3 = np.zeros_like(self.W3)\n",
    "        self.dl_db3 = np.zeros_like(self.b3)\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = X.dot(self.W1) + self.b1 #NxH\n",
    "        X2 = np.maximum(0.,X1) #NxH\n",
    "        O = X2.dot(self.W3) + self.b3 #NxC\n",
    "    \n",
    "        return X,X1,X2,O\n",
    "    \n",
    "    def backward(self,dl_dO, O, X2, X1, X0):\n",
    "        \n",
    "        #backpropagation of dl_dO through last fully connected layer\n",
    "        dl_dX2 = ???\n",
    "        self.dl_dW3 += ???\n",
    "        self.dl_db3 += ???\n",
    "        \n",
    "        #backpropagation of dl_dX2 through ReLU\n",
    "        dl_dX1 = ???\n",
    "        \n",
    "        #backpropagation of dl_dX1 through first fully connected layer\n",
    "        self.dl_dW1 += ???\n",
    "        self.dl_db1 += ???\n",
    "        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail : implémenter la fonction `backward` en utilisant les équations obtenues en TP.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de coût :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x):\n",
    "    x_shift = x - np.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - np.log(np.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def multinoulliCrossEntropyLoss(O, y):\n",
    "    N = y.shape[0]\n",
    "    P = softmax(O.astype('double'))\n",
    "    log_p = logsoftmax(O.astype('double'))\n",
    "    a = log_p[np.arange(N),y]\n",
    "    l = -a.sum()/N\n",
    "    dl_do = P\n",
    "    dl_do[np.arange(N),y] -= 1\n",
    "    dl_do = dl_do/N\n",
    "    return (l, dl_do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode de descente de gradient avec moment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentWithMomentum:\n",
    "    def __init__(self, model, beta, lr):\n",
    "        \n",
    "        self.model = model\n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = np.zeros_like(self.model.W1)\n",
    "        self.Vb1 = np.zeros_like(self.model.b1)\n",
    "        self.VW3 = np.zeros_like(self.model.W3)\n",
    "        self.Vb3 = np.zeros_like(self.model.b3)\n",
    "        \n",
    "    def step(self):\n",
    "        self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.model.dl_dW1\n",
    "        self.model.W1 -= self.lr*self.VW1\n",
    "\n",
    "        self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.model.dl_dW3\n",
    "        self.model.W3 -= self.lr*self.VW3\n",
    "    \n",
    "        self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.model.dl_db1\n",
    "        self.model.b1 -= self.lr*self.Vb1\n",
    "    \n",
    "        self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.model.dl_db3\n",
    "        self.model.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.model.dl_dW1.fill(0.)\n",
    "        self.model.dl_db1.fill(0.)\n",
    "        self.model.dl_dW3.fill(0.)\n",
    "        self.model.dl_db3.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout d'une fonction d'affichage qui va permettre d'afficher les frontières de décisions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    model: neural network\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    _,_,_,O = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant de lancer un apprentissage, il faut choisir les hyper-paramètres de l'architecture et de l'algorithme de descente de gradient (ici nous utilisons une version de l'algorithme de descente de gradient appelée \"momentum\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HYPERPARAMETERS\n",
    "H = 300\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance du MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance de la descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescentWithMomentum(model, beta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons lancer l'optimisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    X0,X1,X2,O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [l, dl_dO] = multinoulliCrossEntropyLoss(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    acc = (np.argmax(O, axis=1) == y).astype('float').sum()/N\n",
    "    print('Iter {} | Training Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_gradients()\n",
    "    model.backward(dl_dO, O, X2, X1, X0)\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(np.mod(i,10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "* Après avoir implémenté la fonction `backward`, vous devriez constater que l'apprentissage sur l'exemple jouet \"fonctionne\", c'est-à-dire que le coût diminue progressivement vers zéro et le taux de bonne classification atteint rapidement 100%.\n",
    "* Vous remarquerez que les paramètres du MLP sont initialisés aléatoirement selon une distribution uniforme centrée en zéro. Remplacer cette initialisation en initialisant tous les paramètres à zéro. Afficher les valeurs des activations (`X1`, `X2`, `O`) et des dérivées (notamment `dl_dW3` et `dl_dW2`). Que constatez-vous ? Pourquoi ?\n",
    "* Restaurer l'initialisation aléatoire des paramètres.\n",
    "* Modifier l'implémentation pour obtenir un MLP à 2 couches cachées (rajouter une transformation affine et une ReLU au MLP actuel).\n",
    "* Modifier l'implémentation pour la rendre générique de telle sorte que le nombre de couches cachées soit un hyper-paramètre réglable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
