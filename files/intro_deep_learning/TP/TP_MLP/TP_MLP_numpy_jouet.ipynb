{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP : Implémentation d'un réseau de neurones de type Perceptron multicouche pour un problème de classification\n",
    "Dans ce TP, vous allez implémenter en Numpy un réseau de neurones de type Perceptron multicouche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder.  \n",
    "\n",
    "---\n",
    "---\n",
    "## Ne pas oublier de configurer Spyder en suivant ces [instructions](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier [utils.py](https://gbourmaud.github.io/files/intro_deep_learning/TP/TP_MLP/utils.py) contient des fonctions qui seront utilisées par la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Définition d'un exemple jouet à 3 classes en 2D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = np.array([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "X -= X.mean() #centering data (globally)\n",
    "X /= X.std() #reduce data (globally)\n",
    "y = np.array([0,0,1,1,2])\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = utils.make_meshgrid(X[:,0], X[:,1], h=0.05)\n",
    "\n",
    "\n",
    "fig1, axs1 = plt.subplots(ncols=2)\n",
    "axs1[0].set_xlim(xx.min(), xx.max())\n",
    "axs1[0].set_ylim(yy.min(), yy.max())\n",
    "axs1[0].grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[y==i,:]\n",
    "    axs1[0].plot(x_c[:,0],x_c[:,1],style_per_class[i],markersize=7, markeredgewidth=3.)\n",
    "\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des fonctions $\\text{FC}$, $\\widetilde{\\text{FC}}$, $\\text{ReLU}$ et $\\widetilde{\\text{ReLU}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_forward(X,W,b):\n",
    "    Z = X.dot(W) + b #NxH\n",
    "    return Z\n",
    "\n",
    "def FC_backward(dc_dZ, X, W, b):\n",
    "    dc_dX = 0 #TODO compute dc_dX (one line)\n",
    "    dc_dW = 0 #TODO compute dc_dW (one line)\n",
    "    dc_db = 0 #TODO compute dc_db (one line)\n",
    "    return dc_dX, dc_dW, dc_db\n",
    "\n",
    "def relu_forward(X):\n",
    "    Z = np.maximum(0.,X) \n",
    "    return Z\n",
    "\n",
    "def relu_backward(dc_dZ, X):\n",
    "    dc_dX = 0 #TODO compute dc_dX (two lines)\n",
    "    return dc_dX\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail : implémenter les fonctions `FC_backward` et  `relu_backward` en utilisant les équations obtenues en TP. Le code ci-après permet de tester vos implémentations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FC_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    H = 20\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "    W = np.random.normal(size=(D,H))\n",
    "    b = np.random.normal(size=(H))\n",
    "    dW = np.random.normal(size=(D,H))\n",
    "    db = np.random.normal(size=(H))\n",
    "\n",
    "\n",
    "    ddX_approx = (FC_forward(X+eta*dX,W,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    dc_dX, _, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddX = (dc_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test FC_backward dl_dX: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dX: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddW_approx = (FC_forward(X,W+eta*dW,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, dc_dW, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddW = (dc_dW*dW).sum()\n",
    "    if(np.isclose(ddW,ddW_approx)):\n",
    "        print('Test FC_backward dl_dW: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dW: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddb_approx = (FC_forward(X,W,b+eta*db).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, _, dc_db = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddb = (dc_db*db).sum()\n",
    "    if(np.isclose(ddb,ddb_approx)):\n",
    "        print('Test FC_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_FC_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "\n",
    "    ddX_approx = (relu_forward(X+eta*dX).sum() - relu_forward(X).sum())/eta\n",
    "    dc_dX = relu_backward(np.ones((N,D)), X)\n",
    "    ddX = (dc_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test relu_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test relu_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_relu_backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, H):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "\n",
    "        #parameters\n",
    "        self.W1 = (np.sqrt(6./self.D))*(2*(np.random.uniform(size=(self.D,self.H))-0.5))\n",
    "        self.b1 = (1./np.sqrt(self.D))*(2*(np.random.uniform(size=(self.H))-0.5))\n",
    "        self.W3 = (np.sqrt(6./self.H))*(2*(np.random.uniform(size=(self.H,self.C))-0.5))\n",
    "        self.b3 = (1./np.sqrt(self.H))*(2*(np.random.uniform(size=(self.C))-0.5))\n",
    "        \n",
    "        #gradients\n",
    "        self.dc_dW1 = np.zeros_like(self.W1)\n",
    "        self.dc_db1 = np.zeros_like(self.b1)\n",
    "        self.dc_dW3 = np.zeros_like(self.W3)\n",
    "        self.dc_db3 = np.zeros_like(self.b3)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = FC_forward(X, self.W1, self.b1) #NxH\n",
    "        X2 = relu_forward(X1) #NxH\n",
    "        S = FC_forward(X2, self.W3, self.b3) #NxC\n",
    "    \n",
    "        return X,X1,X2,S\n",
    "    \n",
    "    def backward(self,dc_dS, S, X2, X1, X0):\n",
    "        \n",
    "        dc_dX2, dc_dW3, dc_db3 = FC_backward(dc_dS, X2, self.W3, self.b3)\n",
    "        self.dc_dW3 += dc_dW3\n",
    "        self.dc_db3 += dc_db3\n",
    "        \n",
    "        dc_dX1 = relu_backward(dc_dX2, X1)\n",
    "        \n",
    "        dc_dX0, dc_dW1, dc_db1 = FC_backward(dc_dX1, X0, self.W1, self.b1)\n",
    "        self.dc_dW1 += dc_dW1\n",
    "        self.dc_db1 += dc_db1\n",
    "        \n",
    "        \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de coût :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x):\n",
    "    x_shift = x - np.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - np.log(np.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def crossEntropyLoss(S, y):\n",
    "    N = y.shape[0]\n",
    "    P = softmax(S.astype('double'))\n",
    "    log_p = logsoftmax(S.astype('double'))\n",
    "    a = log_p[np.arange(N),y]\n",
    "    l = -a.sum()/N\n",
    "    dc_dS = P\n",
    "    dc_dS[np.arange(N),y] -= 1\n",
    "    dc_dS = dc_dS/N\n",
    "    return (l, dc_dS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode de descente de gradient avec moment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentWithMomentum:\n",
    "    def __init__(self, model, beta, lr):\n",
    "        \n",
    "        self.model = model\n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = np.zeros_like(self.model.W1)\n",
    "        self.Vb1 = np.zeros_like(self.model.b1)\n",
    "        self.VW3 = np.zeros_like(self.model.W3)\n",
    "        self.Vb3 = np.zeros_like(self.model.b3)\n",
    "        \n",
    "    def step(self):\n",
    "        self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.model.dc_dW1\n",
    "        self.model.W1 -= self.lr*self.VW1\n",
    "\n",
    "        self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.model.dc_dW3\n",
    "        self.model.W3 -= self.lr*self.VW3\n",
    "    \n",
    "        self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.model.dc_db1\n",
    "        self.model.b1 -= self.lr*self.Vb1\n",
    "    \n",
    "        self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.model.dc_db3\n",
    "        self.model.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.model.dc_dW1.fill(0.)\n",
    "        self.model.dc_db1.fill(0.)\n",
    "        self.model.dc_dW3.fill(0.)\n",
    "        self.model.dc_db3.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant de lancer un apprentissage, il faut choisir les hyper-paramètres de l'architecture et de l'algorithme de descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HYPERPARAMETERS\n",
    "H = 300\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance du MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance de la descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescentWithMomentum(model, beta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons lancer l'optimisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_seq = []\n",
    "it_seq = []\n",
    "line_loss, = axs1[1].plot(it_seq,c_seq)\n",
    "axs1[1].legend()\n",
    "axs1[1].set_xlabel('Iterations')\n",
    "\n",
    "it = 0\n",
    "while 1:    \n",
    "    #Forward Pass\n",
    "    X0,X1,X2,S = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [c, dc_dS] = crossEntropyLoss(S, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = np.argmax(S, axis=1)\n",
    "    acc = (np.argmax(S, axis=1) == y).astype('float').sum()/N\n",
    "    print('Iter {} | Training Loss = {} | Training Accuracy = {}%'.format(it,c,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_gradients()\n",
    "    model.backward(dc_dS, S, X2, X1, X0)\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "    it += 1\n",
    "    \n",
    "    c_seq.append(c)\n",
    "    it_seq.append(it)\n",
    "    if(np.mod(it,10)==0):\n",
    "        #Plot decision boundary\n",
    "        axs1[0].cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[y==i,:]\n",
    "            axs1[0].plot(x_c[:,0],x_c[:,1],style_per_class[i],markersize=7, markeredgewidth=3.)\n",
    "        utils.plot_contours(axs1[0], model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        \n",
    "        line_loss.remove()\n",
    "        line_loss, = axs1[1].plot(it_seq,c_seq,'r',label='Training loss')\n",
    "        axs1[1].legend()\n",
    "        fig1.canvas.draw()\n",
    "        fig1.canvas.flush_events()\n",
    "        plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "* Après avoir implémenté la fonction `backward`, vous devriez constater que l'apprentissage sur l'exemple jouet \"fonctionne\", c'est-à-dire que le coût diminue progressivement vers zéro et le taux de bonne classification atteint rapidement 100%.\n",
    "* Vous remarquerez que les paramètres du MLP sont initialisés aléatoirement selon une distribution uniforme centrée en zéro appelée \"Initialisation de Kaiming\". Remplacer cette initialisation en initialisant tous les paramètres à zéro. Afficher les valeurs des activations (`X1`, `X2`, `O`) et des dérivées (notamment `dl_dW3` et `dl_dW2`). Que constatez-vous ? Pourquoi ?\n",
    "* Restaurer l'initialisation aléatoire des paramètres.\n",
    "* Modifier l'implémentation pour obtenir un MLP à 2 couches cachées (rajouter une transformation affine et une ReLU au MLP actuel).\n",
    "* Modifier l'implémentation pour la rendre générique de telle sorte que le nombre de couches cachées soit un hyper-paramètre réglable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
