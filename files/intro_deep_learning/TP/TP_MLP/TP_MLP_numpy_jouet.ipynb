{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP : Implémentation d'un réseau de neurones de type Perceptron multicouche pour un problème de classification\n",
    "Dans ce TP, vous allez implémenter en Numpy un réseau de neurones de type Perceptron multicouche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "#### 4) Configurer Spyder en suivant ces instructions [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Définition d'un exemple jouet à 3 classes en 2D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdBUlEQVR4nO3df2zdVf0/8Fe3dR3VtYh1rGMdTNBpgpK5gYwwGJpVZiTgohFnlpGIkR9D50zMkDA6JgoGERMUJRow0X0kgQ01EEIT9wNk6CAjooQpCm6yTRhCO9fQle18/+i3ZbVdt47ee3r7fjySZrzf99ye89yJ7pn7ft97q1JKKQAAMhiTewEAQHEpIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGQzLvcCBnPw4MHYuXNnTJw4MaqqqnIvBwA4Ciml2Lt3b0yZMiXGjBn8NY8RXUR27twZTU1NuZcBAByDHTt2xNSpUwcdM6KLyMSJEyOiO0hdXV1Z5uzq6opHHnkkmpubo7q6uixz5lS0vBHFy1y0vBHFy1y0vBHFy1xpedvb26Opqan33/HBjOgi0nM5pq6urqxFpLa2Nurq6ipis9+uouWNKF7mouWNKF7mouWNKF7mSs17NLdVuFkVAMhGEQEAslFEAIBsFBEAIBtFBADIpqRF5Dvf+U6ceeaZMXHixJg0aVJccsklsW3btlJOCQBUkJIWkY0bN8bVV18dTzzxRLS2tsabb74Zzc3NsW/fvlJOCwBUiJJ+jsjDDz/c5/juu++OSZMmxVNPPRXnnXdeKacGACpAWe8RaWtri4iIE044oZzTAgAjVNk+WTWlFMuXL49zzz03Tj/99AHHdHZ2RmdnZ+9xe3t7RHR/olxXV1dZ1tkzT7nmy61oeSOKl7loeSOKl7loeSOKl7nS8g5lnVUppVTCtfS6+uqr48EHH4zHHnvssF+A09LSEqtWrep3fs2aNVFbW1vqJQIAw6CjoyMWLVoUbW1tR/yKlrIUkWuuuSYeeOCB2LRpU0yfPv2w4wZ6RaSpqSn27NlT1u+aaW1tjfnz51fU5/kfq6LljShe5qLljShe5qLljShe5krL297eHg0NDUdVREp6aSalFNdcc02sW7cuNmzYMGgJiYioqamJmpqafuerq6vL/hefY86cipY3oniZi5Y3oniZi5Y3oniZKyXvUNZY0iJy9dVXx5o1a+LXv/51TJw4MXbv3h0REfX19XHccceVcmoAoAKU9F0zd955Z7S1tcW8efOisbGx9+fee+8t5bQAQIUo+aUZAIDD8V0zAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAlABWloiVq8e+LGbbhoT//d/M8q6HhguighABRg7NmLlyv5lZPXqiFWrxsaYMSnPwuBtGpd7AQAc2fXXd/+5cuVbx6tXdx/fcMOBmDnzrxFxWrb1wbFSRAAqxKFl5Fvfiti/P+LGGyNWrDgYDz2Ud21wrFyaAagg118fMX58dwkZP/6tcgKVShEBqCCrV79VQvbvP/wNrFApFBGACtFzT8iNN0Z0dnb/uXJl97tmoFK5RwSgAhxaQnoux7x1z8jY+Pzn3x+f/GS+9cGxUkQAKsCBA31LSI/rr484cOBAPPdcVZ6FwdukiABUgJaWwz923XUH46GHtkXEqeVaDgwbFxYBgGwUEQAgG0UEAMhGEQEAslFEAIBsSlpENm3aFBdddFFMmTIlqqqq4oEHHijldABAhSlpEdm3b1+cccYZcccdd5RyGgCgQpX0c0QWLFgQCxYsKOUUAEAFG1EfaNbZ2RmdnZ29x+3t7RER0dXVFV1dXWVZQ8885Zovt6LljShe5qLljShe5qLljShe5krLO5R1VqWUUgnX8tZEVVWxbt26uOSSSw47pqWlJVatWtXv/Jo1a6K2traEqwMAhktHR0csWrQo2traoq6ubtCxI6qIDPSKSFNTU+zZs+eIQYZLV1dXtLa2xvz586O6urosc+ZUtLwRxctctLwRxctctLwRxctcaXnb29ujoaHhqIrIiLo0U1NTEzU1Nf3OV1dXl/0vPsecORUtb0TxMhctb0TxMhctb0TxMldK3qGs0eeIAADZlPQVkf/+97/x/PPP9x6/8MIL8fTTT8cJJ5wQ06ZNK+XUAEAFKGkRefLJJ+OCCy7oPV6+fHlERCxZsiTuueeeUk4NAFSAkhaRefPmRZnuhQUAKpB7RACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgm3G5FwAjwoEDEY8+GrFrV0RjY8TcuRFjx+ZeFcCop4jA2rURX/1qxL/+9da5qVMjfvCDiIUL860LoABcmqHY1q6N+Mxn+paQiIiXXuo+v3ZtnnUBFIQiQnEdOND9SkhK/R/rObdsWfc4AEpCEaGwqh57rP8rIYdKKWLHju57RwAoCUWE4tq1a3jHATBkigjF1dg4vOMAGDJFhMJK557b/e6YqqqBB1RVRTQ1db+VF4CSUEQorrFju9+iG9G/jPQc3367zxMBKCFFhGJbuDDivvsiTjqp7/mpU7vP+xwRgJLygWawcGHExRf7ZFWADBQRiOguHfPm5V4FQOG4NAMAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANmUpYj86Ec/iunTp8eECRNi1qxZ8eijj5ZjWgBghCt5Ebn33ntj2bJlcd1118XWrVtj7ty5sWDBgti+fXuppwYARriSF5HbbrstvvjFL8bll18eH/zgB+P222+PpqamuPPOO0s9NQAwwo0r5S/fv39/PPXUU7FixYo+55ubm+Pxxx/vN76zszM6Ozt7j9vb2yMioqurK7q6ukq51F4985RrvtyKljeieJmLljeieJmLljeieJkrLe9Q1lmVUkqlWsjOnTvjpJNOit///vdxzjnn9J7/9re/HT//+c9j27Ztfca3tLTEqlWr+v2eNWvWRG1tbamWCQAMo46Ojli0aFG0tbVFXV3doGNL+opIj6qqqj7HKaV+5yIirr322li+fHnvcXt7ezQ1NUVzc/MRgwyXrq6uaG1tjfnz50d1dXVZ5sypaHkjipe5aHkjipe5aHkjipe50vL2XNE4GiUtIg0NDTF27NjYvXt3n/Mvv/xynHjiif3G19TURE1NTb/z1dXVZf+LzzFnTkXLG1G8zEXLG1G8zEXLG1G8zJWSdyhrLOnNquPHj49Zs2ZFa2trn/Otra19LtUAAMVU8kszy5cvj8WLF8fs2bNjzpw5cdddd8X27dvjiiuuKPXUAMAIV/Ii8rnPfS5effXVuPHGG2PXrl1x+umnx0MPPRQnn3xyqacGAEa4stysetVVV8VVV11VjqkAgAriu2YAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyKcuX3gHAgA4ciHj00YhduyIaGyPmzo0YOzb3qigjRQSAPNaujfjqVyP+9a+3zk2dGvGDH0QsXJhvXZSVSzMAlN/atRGf+UzfEhIR8dJL3efXrs2zLspOEQGgvA4c6H4lJKX+j/WcW7asexyjniICQHk9+mj/V0IOlVLEjh3d4xj1FBEAymvXruEdR0VTRAAor8bG4R1HRVNEACivuXO73x1TVTXw41VVEU1N3eMY9RQRAMpr7Njut+hG9C8jPce33+7zRApCEQGg/BYujLjvvoiTTup7furU7vM+R6QwfKAZAHksXBhx8cU+WbXgFBEA8hk7NmLevNyrICOXZgCAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGxKWkRuuummOOecc6K2tjaOP/74Uk4FAFSgkhaR/fv3x2c/+9m48sorSzkNAFChxpXyl69atSoiIu65555STgMAVKiSFpGh6uzsjM7Ozt7j9vb2iIjo6uqKrq6usqyhZ55yzZdb0fJGFC9z0fJGFC9z0fJGFC9zpeUdyjqrUkqphGuJiO5XRJYtWxavv/76oONaWlp6X0U51Jo1a6K2trZEqwMAhlNHR0csWrQo2traoq6ubtCxQ35F5HBl4VBbtmyJ2bNnD/VXx7XXXhvLly/vPW5vb4+mpqZobm4+YpDh0tXVFa2trTF//vyorq4uy5w5FS1vRPEyFy1vRPEyFy1vRPEyV1renisaR2PIRWTp0qVx6aWXDjrmlFNOGeqvjYiImpqaqKmp6Xe+urq67H/xOebMqWh5I4qXuWh5I4qXuWh5I4qXuVLyDmWNQy4iDQ0N0dDQMNSnAQD0U9KbVbdv3x7/+c9/Yvv27XHgwIF4+umnIyLitNNOi3e+852lnBoAqAAlLSIrV66Mn//8573HM2fOjIiI9evXx7x580o5NQBQAUr6gWb33HNPpJT6/SghAECE75oBADJSRACAbBQRACAbRQQAyGZEfdcMxbC9bXvs6dhz2McbahtiWv20Mq4IgFwUEcpqe9v2mHHHjHjjzTcOO2bCuAmxbek2ZQSgAFyaoaz2dOwZtIRERLzx5huDvmICwOihiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIZdVQ2xATxk0YdMyEcROiobahTCsCICcfaEZZTaufFtuWbvPJqgBEhCJCBtPqpykaAESESzMAQEaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQjSICAGSjiAAA2SgiQMVoaYlYvXrgx1av7n4cqCyKCFAxxo6NWLmyfxlZvbr7/NixedYFHLtxuRcAcLSuv777z5Ur3zruKSE33vjW40DlUESAinJoGfnWtyL271dCoJK5NANUnOuvjxg/vruEjB+vhEAlU0SAirN69VslZP/+w9/ACox8ighQUQ69J6Szs/vPgW5gBSqDe0SAijHQjakD3cAKVA5FBKgYBw4MfGNqz/GBA+VfE/D2KCJAxRjsA8u8EgKVyT0iAEA2iggAkI0iAgBko4gAANkoIgBANooIAJCNIgIAZKOIAADZKCIAQDaKCACQTcmKyIsvvhhf/OIXY/r06XHcccfFqaeeGjfccEPs37+/VFMCABWmZN8189xzz8XBgwfjJz/5SZx22mnx5z//Ob70pS/Fvn374tZbby3VtABABSlZEbnwwgvjwgsv7D1+73vfG9u2bYs777xTEQEAIqLM377b1tYWJ5xwwmEf7+zsjM7Ozt7j9vb2iIjo6uqKrq6ukq+vZ65D/xztipY3oniZi5Y3oniZi5Y3oniZKy3vUNZZlVJKJVxLr7///e/xkY98JL73ve/F5ZdfPuCYlpaWWLVqVb/za9asidra2lIvEQAYBh0dHbFo0aJoa2uLurq6QccOuYgcriwcasuWLTF79uze4507d8b5558f559/fvz0pz897PMGekWkqakp9uzZc8Qgw6WrqytaW1tj/vz5UV1dXZY5cypa3ojiZS5a3ojiZS5a3ojiZa60vO3t7dHQ0HBURWTIl2aWLl0al1566aBjTjnllN7/3rlzZ1xwwQUxZ86cuOuuuwZ9Xk1NTdTU1PQ7X11dXfa/+Bxz5lS0vBHFy1y0vBHFy1y0vBHFy1wpeYeyxiEXkYaGhmhoaDiqsS+99FJccMEFMWvWrLj77rtjzBgfWwIAvKVkN6vu3Lkz5s2bF9OmTYtbb701Xnnlld7HJk+eXKppAYAKUrIi8sgjj8Tzzz8fzz//fEydOrXPY2W6PxYAGOFKdq3ksssui5TSgD8AABG+awYAyEgRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACAbRQQAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbMblXsBgUkoREdHe3l62Obu6uqKjoyPa29ujurq6bPPmUrS8EcXLXLS8EcXLXLS8EcXLXGl5e/7d7vl3fDAjuojs3bs3IiKampoyrwQAGKq9e/dGfX39oGOq0tHUlUwOHjwYO3fujIkTJ0ZVVVVZ5mxvb4+mpqbYsWNH1NXVlWXOnIqWN6J4mYuWN6J4mYuWN6J4mSstb0op9u7dG1OmTIkxYwa/C2REvyIyZsyYmDp1apa56+rqKmKzh0vR8kYUL3PR8kYUL3PR8kYUL3Ml5T3SKyE93KwKAGSjiAAA2Sgi/6OmpiZuuOGGqKmpyb2Usiha3ojiZS5a3ojiZS5a3ojiZR7NeUf0zaoAwOjmFREAIBtFBADIRhEBALJRRACAbBSRiLjpppvinHPOidra2jj++OOP6jmXXXZZVFVV9fk5++yzS7vQYXIseVNK0dLSElOmTInjjjsu5s2bF3/5y19Ku9Bh9Nprr8XixYujvr4+6uvrY/HixfH6668P+pxK2uMf/ehHMX369JgwYULMmjUrHn300UHHb9y4MWbNmhUTJkyI9773vfHjH/+4TCsdPkPJvGHDhn57WVVVFc8991wZV3zsNm3aFBdddFFMmTIlqqqq4oEHHjjicyp5j4eat9L39zvf+U6ceeaZMXHixJg0aVJccsklsW3btiM+r5L3+FCKSETs378/PvvZz8aVV145pOddeOGFsWvXrt6fhx56qEQrHF7Hkve73/1u3HbbbXHHHXfEli1bYvLkyTF//vze7wMa6RYtWhRPP/10PPzww/Hwww/H008/HYsXLz7i8yphj++9995YtmxZXHfddbF169aYO3duLFiwILZv3z7g+BdeeCE++clPxty5c2Pr1q3xzW9+M77yla/E/fffX+aVH7uhZu6xbdu2Pvv5vve9r0wrfnv27dsXZ5xxRtxxxx1HNb7S93ioeXtU6v5u3Lgxrr766njiiSeitbU13nzzzWhubo59+/Yd9jmVvsd9JHrdfffdqb6+/qjGLlmyJF188cUlXU+pHW3egwcPpsmTJ6ebb76599wbb7yR6uvr049//OMSrnB4PPvssyki0hNPPNF7bvPmzSki0nPPPXfY51XKHp911lnpiiuu6HPuAx/4QFqxYsWA47/xjW+kD3zgA33OffnLX05nn312ydY43Iaaef369Ski0muvvVaG1ZVWRKR169YNOmY07HGPo8k7mvY3pZRefvnlFBFp48aNhx0zmvbYKyJvw4YNG2LSpEnx/ve/P770pS/Fyy+/nHtJJfHCCy/E7t27o7m5ufdcTU1NnH/++fH4449nXNnR2bx5c9TX18dHP/rR3nNnn3121NfXH3H9I32P9+/fH0899VSfvYmIaG5uPmy2zZs39xv/iU98Ip588sno6uoq2VqHy7Fk7jFz5sxobGyMj3/847F+/fpSLjOrSt/jYzVa9retrS0iIk444YTDjhlNe6yIHKMFCxbEL3/5y/jd734X3/ve92LLli3xsY99LDo7O3Mvbdjt3r07IiJOPPHEPudPPPHE3sdGst27d8ekSZP6nZ80adKg66+EPd6zZ08cOHBgSHuze/fuAce/+eabsWfPnpKtdbgcS+bGxsa466674v7774+1a9fGjBkz4uMf/3hs2rSpHEsuu0rf46EaTfubUorly5fHueeeG6effvphx42mPR7R3777drS0tMSqVasGHbNly5aYPXv2Mf3+z33uc73/ffrpp8fs2bPj5JNPjgcffDAWLlx4TL/z7Sh13oiIqqqqPscppX7nyuloM0f0X3vEkdc/0vZ4MEPdm4HGD3R+JBtK5hkzZsSMGTN6j+fMmRM7duyIW2+9Nc4777ySrjOX0bDHR2s07e/SpUvjT3/6Uzz22GNHHDta9njUFpGlS5fGpZdeOuiYU045Zdjma2xsjJNPPjn+9re/DdvvHIpS5p08eXJEdDfwxsbG3vMvv/xyv0ZeTkeb+U9/+lP8+9//7vfYK6+8MqT1597jgTQ0NMTYsWP7vRIw2N5Mnjx5wPHjxo2Ld7/73SVb63A5lswDOfvss+MXv/jFcC9vRKj0PR4Olbi/11xzTfzmN7+JTZs2xdSpUwcdO5r2eNQWkYaGhmhoaCjbfK+++mrs2LGjzz/U5VTKvNOnT4/JkydHa2trzJw5MyK6r9Nv3LgxbrnllpLMeTSONvOcOXOira0t/vjHP8ZZZ50VERF/+MMfoq2tLc4555yjni/3Hg9k/PjxMWvWrGhtbY1Pf/rTvedbW1vj4osvHvA5c+bMid/+9rd9zj3yyCMxe/bsqK6uLul6h8OxZB7I1q1bR9ReDqdK3+PhUEn7m1KKa665JtatWxcbNmyI6dOnH/E5o2qPs90mO4L885//TFu3bk2rVq1K73znO9PWrVvT1q1b0969e3vHzJgxI61duzallNLevXvT17/+9fT444+nF154Ia1fvz7NmTMnnXTSSam9vT1XjKM21LwppXTzzTen+vr6tHbt2vTMM8+kz3/+86mxsbEi8qaU0oUXXpg+/OEPp82bN6fNmzenD33oQ+lTn/pUnzGVuse/+tWvUnV1dfrZz36Wnn322bRs2bL0jne8I7344osppZRWrFiRFi9e3Dv+H//4R6qtrU1f+9rX0rPPPpt+9rOfperq6nTfffflijBkQ838/e9/P61bty799a9/TX/+85/TihUrUkSk+++/P1eEIdm7d2/v/04jIt12221p69at6Z///GdKafTt8VDzVvr+Xnnllam+vj5t2LAh7dq1q/eno6Ojd8xo2+NDKSKp+22aEdHvZ/369b1jIiLdfffdKaWUOjo6UnNzc3rPe96Tqqur07Rp09KSJUvS9u3b8wQYoqHmTan7Lbw33HBDmjx5cqqpqUnnnXdeeuaZZ8q/+GP06quvpi984Qtp4sSJaeLEiekLX/hCv7f6VfIe//CHP0wnn3xyGj9+fPrIRz7S521/S5YsSeeff36f8Rs2bEgzZ85M48ePT6ecckq68847y7zit28omW+55ZZ06qmnpgkTJqR3vetd6dxzz00PPvhghlUfm563p/7vz5IlS1JKo2+Ph5q30vd3oKz/+//Bo22PD1WV0v+/uwUAoMy8fRcAyEYRAQCyUUQAgGwUEQAgG0UEAMhGEQEAslFEAIBsFBEAIBtFBADIRhEBALJRRACAbBQRACCb/wfZ448+2kIaxAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = np.array([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "y = np.array([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des fonctions $\\text{FC}$, $\\widetilde{\\text{FC}}$, $\\text{ReLU}$ et $\\widetilde{\\text{ReLU}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_forward(X,W,b):\n",
    "    Z = X.dot(W) + b #NxH\n",
    "    return Z\n",
    "\n",
    "def FC_backward(dl_dZ, X, W, b):\n",
    "    dl_dX = 0#TODO\n",
    "    dl_dW = 0#TODO\n",
    "    dl_db = 0#TODO\n",
    "    return dl_dX, dl_dW, dl_db\n",
    "\n",
    "def relu_forward(X):\n",
    "    Z = np.maximum(0.,X)\n",
    "    return Z\n",
    "\n",
    "def relu_backward(dl_dZ, X):\n",
    "    dl_dX = 0#TODO\n",
    "    return dl_dX\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail : implémenter les fonctions `FC_backward` et  `relu_backward` en utilisant les équations obtenues en TP. Le code ci-après permet de tester vos implémentations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def test_FC_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    H = 20\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "    W = np.random.normal(size=(D,H))\n",
    "    b = np.random.normal(size=(H))\n",
    "    dW = np.random.normal(size=(D,H))\n",
    "    db = np.random.normal(size=(H))\n",
    "\n",
    "\n",
    "    ddX_approx = (FC_forward(X+eta*dX,W,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    dl_dX, _, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddX = (dl_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test FC_backward dl_dX: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dX: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddW_approx = (FC_forward(X,W+eta*dW,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, dl_dW, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddW = (dl_dW*dW).sum()\n",
    "    if(np.isclose(ddW,ddW_approx)):\n",
    "        print('Test FC_backward dl_dW: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dW: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddb_approx = (FC_forward(X,W,b+eta*db).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, _, dl_db = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddb = (dl_db*db).sum()\n",
    "    if(np.isclose(ddb,ddb_approx)):\n",
    "        print('Test FC_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_FC_backward()\n",
    "\n",
    "def test_relu_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "\n",
    "    ddX_approx = (relu_forward(X+eta*dX).sum() - relu_forward(X).sum())/eta\n",
    "    dl_dX = relu_backward(np.ones((N,D)), X)\n",
    "    ddX = (dl_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test relu_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test relu_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_relu_backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    def __init__(self, H):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "\n",
    "        #parameters\n",
    "        self.W1 = (np.sqrt(6./self.D))*(2*(np.random.uniform(size=(self.D,self.H))-0.5))\n",
    "        self.b1 = (1./np.sqrt(self.D))*(2*(np.random.uniform(size=(self.H))-0.5))\n",
    "        self.W3 = (np.sqrt(6./self.H))*(2*(np.random.uniform(size=(self.H,self.C))-0.5))\n",
    "        self.b3 = (1./np.sqrt(self.H))*(2*(np.random.uniform(size=(self.C))-0.5))\n",
    "        \n",
    "        #gradients\n",
    "        self.dl_dW1 = np.zeros_like(self.W1)\n",
    "        self.dl_db1 = np.zeros_like(self.b1)\n",
    "        self.dl_dW3 = np.zeros_like(self.W3)\n",
    "        self.dl_db3 = np.zeros_like(self.b3)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = FC_forward(X, self.W1, self.b1) #NxH\n",
    "        X2 = relu_forward(X1) #NxH\n",
    "        O = FC_forward(X2, self.W3, self.b3) #NxC\n",
    "    \n",
    "        return X,X1,X2,O\n",
    "    \n",
    "    def backward(self,dl_dO, O, X2, X1, X0):\n",
    "        \n",
    "        dl_dX2, dl_dW3, dl_db3 = FC_backward(dl_dO, X2, self.W3, self.b3)\n",
    "        self.dl_dW3 += dl_dW3\n",
    "        self.dl_db3 += dl_db3\n",
    "        \n",
    "        dl_dX1 = relu_backward(dl_dX2, X1)\n",
    "        \n",
    "        dl_dX0, dl_dW1, dl_db1 = FC_backward(dl_dX1, X0, self.W1, self.b1)\n",
    "        self.dl_dW1 += dl_dW1\n",
    "        self.dl_db1 += dl_db1\n",
    "        \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de coût :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x):\n",
    "    x_shift = x - np.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - np.log(np.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def multinoulliCrossEntropyLoss(O, y):\n",
    "    N = y.shape[0]\n",
    "    P = softmax(O.astype('double'))\n",
    "    log_p = logsoftmax(O.astype('double'))\n",
    "    a = log_p[np.arange(N),y]\n",
    "    l = -a.sum()/N\n",
    "    dl_do = P\n",
    "    dl_do[np.arange(N),y] -= 1\n",
    "    dl_do = dl_do/N\n",
    "    return (l, dl_do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode de descente de gradient avec moment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentWithMomentum:\n",
    "    def __init__(self, model, beta, lr):\n",
    "        \n",
    "        self.model = model\n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = np.zeros_like(self.model.W1)\n",
    "        self.Vb1 = np.zeros_like(self.model.b1)\n",
    "        self.VW3 = np.zeros_like(self.model.W3)\n",
    "        self.Vb3 = np.zeros_like(self.model.b3)\n",
    "        \n",
    "    def step(self):\n",
    "        self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.model.dl_dW1\n",
    "        self.model.W1 -= self.lr*self.VW1\n",
    "\n",
    "        self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.model.dl_dW3\n",
    "        self.model.W3 -= self.lr*self.VW3\n",
    "    \n",
    "        self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.model.dl_db1\n",
    "        self.model.b1 -= self.lr*self.Vb1\n",
    "    \n",
    "        self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.model.dl_db3\n",
    "        self.model.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.model.dl_dW1.fill(0.)\n",
    "        self.model.dl_db1.fill(0.)\n",
    "        self.model.dl_dW3.fill(0.)\n",
    "        self.model.dl_db3.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout d'une fonction d'affichage qui va permettre d'afficher les frontières de décisions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    model: neural network\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    _,_,_,O = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant de lancer un apprentissage, il faut choisir les hyper-paramètres de l'architecture et de l'algorithme de descente de gradient (ici nous utilisons une version de l'algorithme de descente de gradient appelée \"momentum\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HYPERPARAMETERS\n",
    "H = 300\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance du MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance de la descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescentWithMomentum(model, beta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons lancer l'optimisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    X0,X1,X2,O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [l, dl_dO] = multinoulliCrossEntropyLoss(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    acc = (np.argmax(O, axis=1) == y).astype('float').sum()/N\n",
    "    print('Iter {} | Training Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_gradients()\n",
    "    model.backward(dl_dO, O, X2, X1, X0)\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(np.mod(i,10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "* Après avoir implémenté la fonction `backward`, vous devriez constater que l'apprentissage sur l'exemple jouet \"fonctionne\", c'est-à-dire que le coût diminue progressivement vers zéro et le taux de bonne classification atteint rapidement 100%.\n",
    "* Vous remarquerez que les paramètres du MLP sont initialisés aléatoirement selon une distribution uniforme centrée en zéro. Remplacer cette initialisation en initialisant tous les paramètres à zéro. Afficher les valeurs des activations (`X1`, `X2`, `O`) et des dérivées (notamment `dl_dW3` et `dl_dW2`). Que constatez-vous ? Pourquoi ?\n",
    "* Restaurer l'initialisation aléatoire des paramètres.\n",
    "* Modifier l'implémentation pour obtenir un MLP à 2 couches cachées (rajouter une transformation affine et une ReLU au MLP actuel).\n",
    "* Modifier l'implémentation pour la rendre générique de telle sorte que le nombre de couches cachées soit un hyper-paramètre réglable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
