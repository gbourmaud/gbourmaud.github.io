{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP : Implémentation d'un réseau de neurones de type Perceptron multicouche pour un problème de classification\n",
    "Dans ce TP, vous allez implémenter en Numpy un réseau de neurones de type Perceptron multicouche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder.  \n",
    "\n",
    "---\n",
    "---\n",
    "## Ne pas oublier de configurer Spyder en suivant ces [instructions](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier [utils.py](https://gbourmaud.github.io/files/intro_deep_learning/TP/TP_MLP/utils.py) contient des fonctions qui seront utilisées par la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Définition d'un exemple jouet à 3 classes en 2D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = np.array([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "X -= X.mean() #centering data (globally)\n",
    "X /= X.std() #reduce data (globally)\n",
    "y = np.array([0,0,1,1,2])\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = utils.make_meshgrid(X[:,0], X[:,1], h=0.01)\n",
    "\n",
    "\n",
    "fig1, axs1 = plt.subplots(1)\n",
    "axs1.set_xlim(xx.min(), xx.max())\n",
    "axs1.set_ylim(yy.min(), yy.max())\n",
    "axs1.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[y==i,:]\n",
    "    axs1.plot(x_c[:,0],x_c[:,1],style_per_class[i],markersize=7, markeredgewidth=3.)\n",
    "\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation des fonctions $\\text{FC}$, $\\widetilde{\\text{FC}}$, $\\text{ReLU}$ et $\\widetilde{\\text{ReLU}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_forward(X,W,b):\n",
    "    Z = X.dot(W) + b #NxH\n",
    "    return Z\n",
    "\n",
    "def FC_backward(dc_dZ, X, W, b):\n",
    "    dc_dX = 0 #TODO compute dc_dX (one line)\n",
    "    dc_dW = 0 #TODO compute dc_dW (one line)\n",
    "    dc_db = 0 #TODO compute dc_db (one line)\n",
    "    return dc_dX, dc_dW, dc_db\n",
    "\n",
    "def relu_forward(X):\n",
    "    Z = np.maximum(0.,X) \n",
    "    return Z\n",
    "\n",
    "def relu_backward(dc_dZ, X):\n",
    "    dc_dX = 0 #TODO compute dc_dX (two lines)\n",
    "    return dc_dX\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail : implémenter les fonctions `FC_backward` et  `relu_backward` en utilisant les équations obtenues en TP. Le code ci-après permet de tester vos implémentations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_FC_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    H = 20\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "    W = np.random.normal(size=(D,H))\n",
    "    b = np.random.normal(size=(H))\n",
    "    dW = np.random.normal(size=(D,H))\n",
    "    db = np.random.normal(size=(H))\n",
    "\n",
    "\n",
    "    ddX_approx = (FC_forward(X+eta*dX,W,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    dc_dX, _, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddX = (dc_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test FC_backward dl_dX: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dX: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddW_approx = (FC_forward(X,W+eta*dW,b).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, dc_dW, _ = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddW = (dc_dW*dW).sum()\n",
    "    if(np.isclose(ddW,ddW_approx)):\n",
    "        print('Test FC_backward dl_dW: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_dW: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    ddb_approx = (FC_forward(X,W,b+eta*db).sum() - FC_forward(X,W,b).sum())/eta\n",
    "    _, _, dc_db = FC_backward(np.ones((N,H)), X, W, b)\n",
    "    ddb = (dc_db*db).sum()\n",
    "    if(np.isclose(ddb,ddb_approx)):\n",
    "        print('Test FC_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test FC_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_FC_backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relu_backward():\n",
    "    \n",
    "    eta = 1e-5\n",
    "    N = 100\n",
    "    D = 10\n",
    "    X = np.random.normal(size=(N,D))\n",
    "    dX = np.random.normal(size=(N,D))\n",
    "\n",
    "    ddX_approx = (relu_forward(X+eta*dX).sum() - relu_forward(X).sum())/eta\n",
    "    dc_dX = relu_backward(np.ones((N,D)), X)\n",
    "    ddX = (dc_dX*dX).sum()\n",
    "    if(np.isclose(ddX,ddX_approx)):\n",
    "        print('Test relu_backward dl_db: SUCCESS')\n",
    "    else:\n",
    "        print('Test relu_backward dl_db: FAILURE')\n",
    "        sys.exit()\n",
    "    \n",
    "    return\n",
    "\n",
    "test_relu_backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, H):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "\n",
    "        #parameters\n",
    "        self.W1 = (np.sqrt(6./self.D))*(2*(np.random.uniform(size=(self.D,self.H))-0.5))\n",
    "        self.b1 = (1./np.sqrt(self.D))*(2*(np.random.uniform(size=(self.H))-0.5))\n",
    "        self.W3 = (np.sqrt(6./self.H))*(2*(np.random.uniform(size=(self.H,self.C))-0.5))\n",
    "        self.b3 = (1./np.sqrt(self.H))*(2*(np.random.uniform(size=(self.C))-0.5))\n",
    "        \n",
    "        #gradients\n",
    "        self.dc_dW1 = np.zeros_like(self.W1)\n",
    "        self.dc_db1 = np.zeros_like(self.b1)\n",
    "        self.dc_dW3 = np.zeros_like(self.W3)\n",
    "        self.dc_db3 = np.zeros_like(self.b3)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = FC_forward(X, self.W1, self.b1) #NxH\n",
    "        X2 = relu_forward(X1) #NxH\n",
    "        S = FC_forward(X2, self.W3, self.b3) #NxC\n",
    "    \n",
    "        return X,X1,X2,S\n",
    "    \n",
    "    def backward(self,dc_dS, S, X2, X1, X0):\n",
    "        \n",
    "        dc_dX2, dc_dW3, dc_db3 = FC_backward(dc_dS, X2, self.W3, self.b3)\n",
    "        self.dc_dW3 += dc_dW3\n",
    "        self.dc_db3 += dc_db3\n",
    "        \n",
    "        dc_dX1 = relu_backward(dc_dX2, X1)\n",
    "        \n",
    "        dc_dX0, dc_dW1, dc_db1 = FC_backward(dc_dX1, X0, self.W1, self.b1)\n",
    "        self.dc_dW1 += dc_dW1\n",
    "        self.dc_db1 += dc_db1\n",
    "        \n",
    "        \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de coût :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x):\n",
    "    x_shift = x - np.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - np.log(np.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def crossEntropyLoss(S, y):\n",
    "    N = y.shape[0]\n",
    "    P = softmax(S.astype('double'))\n",
    "    log_p = logsoftmax(S.astype('double'))\n",
    "    a = log_p[np.arange(N),y]\n",
    "    l = -a.sum()/N\n",
    "    dc_dS = P\n",
    "    dc_dS[np.arange(N),y] -= 1\n",
    "    dc_dS = dc_dS/N\n",
    "    return (l, dc_dS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode de descente de gradient avec moment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentWithMomentum:\n",
    "    def __init__(self, model, beta, lr):\n",
    "        \n",
    "        self.model = model\n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = np.zeros_like(self.model.W1)\n",
    "        self.Vb1 = np.zeros_like(self.model.b1)\n",
    "        self.VW3 = np.zeros_like(self.model.W3)\n",
    "        self.Vb3 = np.zeros_like(self.model.b3)\n",
    "        \n",
    "    def step(self):\n",
    "        self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.model.dc_dW1\n",
    "        self.model.W1 -= self.lr*self.VW1\n",
    "\n",
    "        self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.model.dc_dW3\n",
    "        self.model.W3 -= self.lr*self.VW3\n",
    "    \n",
    "        self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.model.dc_db1\n",
    "        self.model.b1 -= self.lr*self.Vb1\n",
    "    \n",
    "        self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.model.dc_db3\n",
    "        self.model.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.model.dc_dW1.fill(0.)\n",
    "        self.model.dc_db1.fill(0.)\n",
    "        self.model.dc_dW3.fill(0.)\n",
    "        self.model.dc_db3.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant de lancer un apprentissage, il faut choisir les hyper-paramètres de l'architecture et de l'algorithme de descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HYPERPARAMETERS\n",
    "H = 300\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance du MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance de la descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescentWithMomentum(model, beta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons lancer l'optimisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "while 1:    \n",
    "    #Forward Pass\n",
    "    X0,X1,X2,S = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [c, dc_dS] = crossEntropyLoss(S, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = np.argmax(S, axis=1)\n",
    "    acc = (np.argmax(S, axis=1) == y).astype('float').sum()/N\n",
    "    print('Iter {} | Training Loss = {} | Training Accuracy = {}%'.format(it,c,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_gradients()\n",
    "    model.backward(dc_dS, S, X2, X1, X0)\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "    it += 1\n",
    "    \n",
    "    \n",
    "    if(np.mod(it,10)==0):\n",
    "        #Plot decision boundary\n",
    "        axs1.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[y==i,:]\n",
    "            axs1.plot(x_c[:,0],x_c[:,1],style_per_class[i],markersize=7, markeredgewidth=3.)\n",
    "        utils.plot_contours(axs1, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "* Après avoir implémenté la fonction `backward`, vous devriez constater que l'apprentissage sur l'exemple jouet \"fonctionne\", c'est-à-dire que le coût diminue progressivement vers zéro et le taux de bonne classification atteint rapidement 100%.\n",
    "* Vous remarquerez que les paramètres du MLP sont initialisés aléatoirement selon une distribution uniforme centrée en zéro appelée \"Initialisation de Kaiming\". Remplacer cette initialisation en initialisant tous les paramètres à zéro. Afficher les valeurs des activations (`X1`, `X2`, `O`) et des dérivées (notamment `dl_dW3` et `dl_dW2`). Que constatez-vous ? Pourquoi ?\n",
    "* Restaurer l'initialisation aléatoire des paramètres.\n",
    "* Modifier l'implémentation pour obtenir un MLP à 2 couches cachées (rajouter une transformation affine et une ReLU au MLP actuel).\n",
    "* Modifier l'implémentation pour la rendre générique de telle sorte que le nombre de couches cachées soit un hyper-paramètre réglable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
