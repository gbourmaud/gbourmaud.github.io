{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP : Implémentation d'un réseau de neurones de type Perceptron multicouche pour un problème de classification\n",
    "Dans ce TP, vous allez implémenter en Numpy un réseau de neurones de type Perceptron multicouche.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "#### 4) Configurer Spyder en suivant ces instructions [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 : Exemple \"jouet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copiez/collez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Définition d'un exemple jouet à 3 classes en 2D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = np.array([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "y = np.array([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'un MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, H):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        #parameters\n",
    "        self.W1 = (np.sqrt(6./self.D))*(2*(np.random.uniform(size=(self.D,self.H))-0.5))\n",
    "        self.b1 = (1./np.sqrt(self.D))*(2*(np.random.uniform(size=(self.H))-0.5))\n",
    "        self.W3 = (np.sqrt(6./self.H))*(2*(np.random.uniform(size=(self.H,self.C))-0.5))\n",
    "        self.b3 = (1./np.sqrt(self.H))*(2*(np.random.uniform(size=(self.C))-0.5))\n",
    "        \n",
    "        \n",
    "        #gradients\n",
    "        self.dl_dW1 = np.zeros_like(self.W1)\n",
    "        self.dl_db1 = np.zeros_like(self.b1)\n",
    "        self.dl_dW3 = np.zeros_like(self.W3)\n",
    "        self.dl_db3 = np.zeros_like(self.b3)\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = X.dot(self.W1) + self.b1 #NxH\n",
    "        X2 = np.maximum(0.,X1) #NxH\n",
    "        O = X2.dot(self.W3) + self.b3 #NxC\n",
    "    \n",
    "        return X,X1,X2,O\n",
    "    \n",
    "    def backward(self,dl_dO, O, X2, X1, X0):\n",
    "        \n",
    "        #backpropagation of dl_dO through last fully connected layer\n",
    "        dl_dX2 = ???\n",
    "        self.dl_dW3 += ???\n",
    "        self.dl_db3 += ???\n",
    "        \n",
    "        #backpropagation of dl_dX2 through ReLU\n",
    "        dl_dX1 = ???\n",
    "        \n",
    "        #backpropagation of dl_dX1 through first fully connected layer\n",
    "        self.dl_dW1 += ???\n",
    "        self.dl_db1 += ???\n",
    "        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Travail : implémenter la fonction `backward` en utilisant les équations obtenues en TP. La sortie du réseau (variable `O`) est-elle utilisée ? Pourquoi ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de coût :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax(x):\n",
    "    x_shift = x - np.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - np.log(np.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def multinoulliCrossEntropyLoss(O, y):\n",
    "    N = y.shape[0]\n",
    "    P = softmax(O.astype('double'))\n",
    "    log_p = logsoftmax(O.astype('double'))\n",
    "    a = log_p[np.arange(N),y]\n",
    "    l = -a.sum()/N\n",
    "    dl_do = P\n",
    "    dl_do[np.arange(N),y] -= 1\n",
    "    dl_do = dl_do/N\n",
    "    return (l, dl_do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de la méthode de descente de gradient avec moment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentWithMomentum:\n",
    "    def __init__(self, model, beta, lr):\n",
    "        \n",
    "        self.model = model\n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = np.zeros_like(self.model.W1)\n",
    "        self.Vb1 = np.zeros_like(self.model.b1)\n",
    "        self.VW3 = np.zeros_like(self.model.W3)\n",
    "        self.Vb3 = np.zeros_like(self.model.b3)\n",
    "        \n",
    "    def step(self):\n",
    "        self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.model.dl_dW1\n",
    "        self.model.W1 -= self.lr*self.VW1\n",
    "\n",
    "        self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.model.dl_dW3\n",
    "        self.model.W3 -= self.lr*self.VW3\n",
    "    \n",
    "        self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.model.dl_db1\n",
    "        self.model.b1 -= self.lr*self.Vb1\n",
    "    \n",
    "        self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.model.dl_db3\n",
    "        self.model.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.model.dl_dW1.fill(0.)\n",
    "        self.model.dl_db1.fill(0.)\n",
    "        self.model.dl_dW3.fill(0.)\n",
    "        self.model.dl_db3.fill(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout d'une fonction d'affichage qui va permettre d'afficher les frontières de décisions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    model: neural network\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    _,_,_,O = model.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avant de lancer un apprentissage, il faut choisir les hyper-paramètres de l'architecture et de l'algorithme de descente de gradient (ici nous utilisons une version de l'algorithme de descente de gradient appelée \"momentum\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% HYPERPARAMETERS\n",
    "H = 300\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance du MLP à une couche cachée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une instance de la descente de gradient avec moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = GradientDescentWithMomentum(model, beta, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous pouvons lancer l'optimisation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    X0,X1,X2,O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [l, dl_dO] = multinoulliCrossEntropyLoss(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = np.argmax(O, axis=1)\n",
    "    acc = (np.argmax(O, axis=1) == y).astype('float').sum()/N\n",
    "    print('Iter {} | Training Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_gradients()\n",
    "    model.backward(dl_dO, O, X2, X1, X0)\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(np.mod(i,10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "* Après avoir implémenté la fonction `backward`, vous devriez constater que l'apprentissage sur l'exemple jouet \"fonctionne\", c'est-à-dire que le coût diminue progressivement vers zéro et le taux de bonne classification atteint rapidement 100%.\n",
    "* Vous remarquerez que les paramètres du MLP sont initialisés aléatoirement selon une distribution uniforme centrée en zéro. Remplacer cette initialisation en initialisant tous les paramètres à zéro. Afficher les valeurs des activations (`X1`, `X2`, `O`) et des dérivées (notamment `dl_dW3` et `dl_dW2`). Que constatez-vous ? Pourquoi ?\n",
    "* Restaurer l'initialisation aléatoire des paramètres.\n",
    "* Modifier l'implémentation pour obtenir un MLP à 2 couches cachées (rajouter une transformation affine et une ReLU au MLP actuel).\n",
    "* (Optionnel) Modifier l'implémentation pour la rendre générique de telle sorte que le nombre de couches cachées soit un hyper-paramètre réglable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 2 : Reconnaissance de chiffres manuscrits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que vous disposez d'une implémentation fonctionnelle de l'optimisation des paramètres d'un MLP sur un exemple \"jouet\", vous allez pouvoir l'adapter à un cas réel (de petite taille pour pouvoir effectuer des apprentissages en quelques dizaines de secondes sur CPU).  \n",
    "\n",
    "La base de données étiquetées MNIST contient 70000 images de chiffres manuscrits. Chaque image est de taille 28x28, et est accompagnée d'une étiquette indiquant le chiffre qu'elle représente (0,1,2,..,9). Ainsi la base de données MNIST est très pratique pour résoudre un problème d'apprentissage supervisé consistant à entraîner un réseau de neurones à reconnaître des chiffres manuscrits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place de la base MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Télécharger la base MNIST [ici](https://gbourmaud.github.io/files/intro_deep_learning/TP/TP_MLP/MNIST_bmp.zip)\n",
    "+ Décompresser le ficher dans `/tmp` **(attention bien décompresser dans `/tmp` et pas dans `/net/.../tmp`)**\n",
    "+ Les fichiers décompressés sont au format .bmp, ainsi vous pouvez les ouvrir pour regarder à quoi ressemble les données de cette base. **Un problème d'apprentissage supervisé repose sur une base de données étiquetées, ainsi il est très important de prendre le temps de visualiser les données et leurs étiquettes pour savoir ce qu'on s'apprête à traiter !**\n",
    "+ Le code suivant permet de charger la base d'entraînement de MNIST (60000 images) ainsi que la base de test de MNIST (10000 images) (la partie `__main__` permet d'afficher quelques images et leurs étiquettes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "from os import path\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_MNIST(path_MNIST_bmp):\n",
    "    \n",
    "    filenames = listdir(path_MNIST_bmp)\n",
    "    \n",
    "    imgs_train = np.zeros((60000,28,28))\n",
    "    labels_train = np.zeros(60000,dtype=np.uint8)\n",
    "    n_train = 0\n",
    "    \n",
    "    imgs_test = np.zeros((10000,28,28))\n",
    "    labels_test = np.zeros(10000,dtype=np.uint8)\n",
    "    n_test = 0\n",
    "    \n",
    "    W=H=28\n",
    "    for i, filename in enumerate(filenames):\n",
    "        \n",
    "        f_name, f_ext = path.splitext(filename)\n",
    "        set_type, class_type ,num_im = f_name.split('_')\n",
    "        \n",
    "        num_im = int(num_im)\n",
    "        img = np.array(Image.open(path.join(path_MNIST_bmp,filename)))/255.\n",
    "        \n",
    "        N_W = math.ceil(math.sqrt(num_im))\n",
    "        N_H = math.ceil(num_im/N_W)\n",
    "        \n",
    "        im_array_ext = img.reshape(N_H,H,W*N_W).transpose((1,0,2))\n",
    "        im_array_ext = im_array_ext.reshape(H,N_W*N_H,W).transpose((1,0,2))\n",
    "\n",
    "        im_array = im_array_ext[:num_im,:,:]        \n",
    "        \n",
    "        if(set_type=='train'):\n",
    "            imgs_train[n_train:n_train+num_im,:,:] = im_array\n",
    "            labels_train[n_train:n_train+num_im] = int(class_type)\n",
    "            n_train += num_im\n",
    "        elif(set_type=='test'):\n",
    "            imgs_test[n_test:n_test+num_im,:,:] = im_array\n",
    "            labels_test[n_test:n_test+num_im] = int(class_type)\n",
    "            n_test += num_im\n",
    "                \n",
    "    assert n_test == 10000\n",
    "    assert n_train == 60000\n",
    "        \n",
    "        \n",
    "    \n",
    "    return imgs_train, labels_train, imgs_test, labels_test\n",
    "    \n",
    "if(__name__ == \"__main__\"):\n",
    "    path_MNIST_bmp = './MNIST_bmp'\n",
    "\n",
    "    imgs_train, labels_train, imgs_test, labels_test = load_MNIST(path_MNIST_bmp)\n",
    "    n_train = imgs_train.shape[0]\n",
    "    n_test = imgs_test.shape[0]\n",
    "    \n",
    "    ids = np.random.permutation(n_train)\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(8):\n",
    "        for j in range(4):\n",
    "            plt.subplot(4,8,i+1 + j*8)\n",
    "            plt.imshow(imgs_train[ids[i+j*8],:,:])\n",
    "            plt.title(labels_train[ids[i+j*8]])\n",
    "            plt.axis('off')\n",
    "\n",
    "    vec_train = imgs_train.reshape((n_train,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travail à effectuer\n",
    "\n",
    "+ Adapter le code de la Partie 1 pour entraîner un MLP à reconnaître des chiffres manuscrits avec une descente de gradient **stochastique** avec **arrêt prématuré** (la fonction `np.random.permutation` permet notamment de mélanger des indices, ce qui est pratique pour mélanger les données avant chaque epoch). **Remarque : Tout ce qui concerne la génération des données de l'exemple jouet de la Partie 1, ainsi que les fonctions d'affichage des frontières de décisions ne doivent pas être conservées.**\n",
    "+ Si votre code fonctionne correctement, en quelques epochs (par exemple une dizaine d'epochs avec une taille de minibatch de 256), vous devriez obtenir un taux de bonne classification supérieur à 90% sur la base de test.\n",
    "+ Modifier les hyperparamètres (pas d'apprentissage, taille du minibatch, nombre de couches du MLP, taille de chaque couche,...) pour essayer d'améliorer le taux de bonne classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
