{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à PyTorch\n",
    "La documentation de la bibliothèque PyTorch est [ici](https://pytorch.org/docs/1.12/ ). Il est également possible d'accéder à la documentation d'une fonction en tapant `help(torch.nom_de_la_fonction)` dans le terminal python de Spyder (exemple : `help(torch.matmul)` après avoir importé la biblitohèque PyTorch `import torch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copier/coller le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Fonctionnalité \"autograd\" de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `torch.tensor` est l'équivalent en PyTorch d'un `numpy.array` en Numpy : il s'agit d'un tableau multidimensionnel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "#Exemple de création d'un tableau bidimensionnel\n",
    "x = torch.tensor([[1, 2, 3],[4, 5 ,6], [7, 8 ,9], [10, 11, 12]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principale différence entre un `numpy.array` et un `torch.tensor` est le fait que le `torch.tensor` permet l'utilisation de la fonctionnalité *autograd* (option `requires_grad=True`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Exemple de création d'un tableau bidimensionnel en activant l'autograd\n",
    "x = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque cette fonctionnalité est activée pour un `torch.tensor` $X$ , un graphe de calcul se crée et chaque opération faisant intervenir (directement ou indirectement) $X$ est ajoutée à ce graphe de calcul. Tout ce processus est transparent pour l'utilisateur. Ce processus de création d'un graphe de calcul correspond à l'étape de **propagation avant** vue en cours.  Lorsque la méthode `.backward()` d'une variable `torch.tensor` $y$ **scalaire** du graphe de calcul, est exécutée, l'étape de **rétropropagation** s'effectue et calcule la dérivée $\\frac{\\partial y}{\\partial X}$. Le résultat est stockée dans le champs `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des scalaires\n",
    "\n",
    "# Création des tenseurs scalaires\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2.)\n",
    "b = torch.tensor(3.)\n",
    "\n",
    "print(x.grad) # None\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z = w*x\n",
    "y = z+b # y = 2*x + 3\n",
    "\n",
    "# Calcul du gradient (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage du gradient\n",
    "print(x.grad)    # x.grad = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dessiner (sur papier) le graphe de calcul de l'exemple précédent.** Dans ce graphe, vous remarquerez qu'il y a trois *feuilles* : le tenseur $x$, le tenseur $w$ et le tenseur $b$. Pour obtenir  $\\frac{\\partial y}{\\partial w}$ et $\\frac{\\partial y}{\\partial b}$ en plus de $\\frac{\\partial y}{\\partial x}$, il suffit d'utiliser l'option `requires_grad=True` sur $w$ et $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "tensor(2.)\n",
      "tensor(4.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des scalaires\n",
    "\n",
    "# Création des tenseurs scalaires\n",
    "x = torch.tensor(4., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "print(x.grad) # None\n",
    "print(w.grad) # None\n",
    "print(b.grad) # None\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z = w*x\n",
    "y = z+b\n",
    "\n",
    "# Calcul des gradients (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage des gradients\n",
    "print(x.grad)    # x.grad = 2.\n",
    "print(w.grad)    # w.grad = 4. \n",
    "print(b.grad)    # b.grad = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce mécanisme d'autograd fonctionne de la même manière lorsque les feuilles sont des tableaux multimensionnels plutôt que des scalaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.]])\n",
      "tensor([[22., 22.],\n",
      "        [26., 26.],\n",
      "        [30., 30.]])\n",
      "tensor([4., 4.])\n"
     ]
    }
   ],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des tenseurs 2D\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "b = torch.tensor([4., 5.], requires_grad=True) #vecteur de taille 2\n",
    "\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z1 = X.matmul(W)\n",
    "z2 = z1 + b\n",
    "y = z2.sum()\n",
    "\n",
    "# Calcul des gradients (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage des gradients\n",
    "print(X.grad)\n",
    "print(W.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dessiner (sur papier) le graphe de calcul de l'exemple précédent.** Quelle devrait-être les tailles des variables `X.grad`, `W.grad` et `b.grad` ? Vérifier leurs tailles dans le code (attribut `.shape` d'un tenseur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous venons de la voir, la fonctionnalité *autograd* est une implémentation du théorème de dérivation d'une fonction composée. Pour s'en convaincre, prenons le cas de la composition de deux fonctions $y=g(Z)$ et $Z=f(X)$. Nous allons comparer deux utilisations de l'autograd :\n",
    "\n",
    "Cas 1) Calculer directement $\\frac{\\partial y}{\\partial X}$ avec l'autograd\n",
    "\n",
    "Cas 2) Calculer manuellement $\\frac{\\partial y}{\\partial Z}$ (variable `dy_dZ`) et fournir ce gradient à l'autograd (`Z.backward(dy_dZ)`) pour qu'il termine le calcule de $\\frac{\\partial y}{\\partial X}$.\n",
    "\n",
    "Les gradients obtenus avec le cas 1 et le cas 2 doivent être parfaitement identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.]])\n",
      "tensor([[ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.],\n",
      "        [ 3.,  9., 15.]])\n"
     ]
    }
   ],
   "source": [
    "# CAS 1\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "Z = X.matmul(W)\n",
    "y = Z.sum()\n",
    "\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "print(X.grad)\n",
    "# CAS 2\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "Z = X.matmul(W)\n",
    "y = Z.sum()\n",
    "\n",
    "dy_dZ = torch.ones(Z.shape) #dérivée de la fonction sum\n",
    "Z.backward(dy_dZ)\n",
    "\n",
    "print(X.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
