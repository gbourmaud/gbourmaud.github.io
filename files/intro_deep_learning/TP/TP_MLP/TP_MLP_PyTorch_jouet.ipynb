{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP MLP PyTorch \n",
    "Dans ce TP, vous allez implémenter un réseau de neurones de type *Perceptron Multicouche* en utilisant la bibliothèque PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### Si vous utilisez un ordinateur de l'Enseirb:\n",
    "#### 1) Lancer une session linux (et non pas windows)\n",
    "#### 2) Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "#### 3) Dans ce terminal, taper la commande suivante pour lancer Spyder :  \n",
    "`spyder &`  \n",
    "#### 4) Configurer Spyder en suivant ces instructions [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "### Si vous utilisez votre ordinateur personnel, il faudra installer Spyder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Introduction à PyTorch\n",
    "La documentation de la bibliothèque PyTorch est [ici](https://pytorch.org/docs/1.12/ ). Il est également possible d'accéder à la documentation d'une fonction en tapant `help(torch.nom_de_la_fonction)` dans le terminal python de Spyder (exemple : `help(torch.matmul)` après avoir importé la biblitohèque PyTorch `import torch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un nouveau script python et copier/coller le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Fonctionnalité \"autograd\" de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un `torch.tensor` est l'équivalent en PyTorch d'un `numpy.array` en Numpy : il s'agit d'un tableau multidimensionnel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de création d'un tableau bidimensionnel\n",
    "x = torch.tensor([[1, 2, 3],[4, 5 ,6], [7, 8 ,9], [10, 11, 12]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principale différence entre un `numpy.array` et un `torch.tensor` est le fait que le `torch.tensor` permet l'utilisation de la fonctionnalité *autograd* (option `requires_grad=True`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de création d'un tableau bidimensionnel en activant l'autograd\n",
    "x = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque cette fonctionnalité est activée pour un `torch.tensor` $X$ , un graphe de calcul se crée et chaque opération faisant intervenir (directement ou indirectement) $X$ est ajoutée à ce graphe de calcul. Tout ce processus est transparent pour l'utilisateur. Ce processus de création d'un graphe de calcul correspond à l'étape de **propagation avant** vue en cours.  Lorsque la méthode `.backward()` d'une variable `torch.tensor` $y$ **scalaire** du graphe de calcul, est exécutée, l'étape de **rétropropagation** s'effectue et calcule la dérivée $\\frac{\\partial y}{\\partial X}$. Le résultat est stockée dans le champs `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des scalaires\n",
    "\n",
    "# Création des tenseurs scalaires\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2.)\n",
    "b = torch.tensor(3.)\n",
    "\n",
    "print(x.grad) # None\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z = w*x\n",
    "y = z+b # y = 2*x + 3\n",
    "\n",
    "# Calcul du gradient (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage du gradient\n",
    "print(x.grad)    # x.grad = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessiner le graphe de calcul de l'exemple précédent. Dans ce graphe, vous remarquerez qu'il y a trois *feuilles* : le tenseur $x$, le tenseur $w$ et le tenseur $b$. Pour obtenir  $\\frac{\\partial y}{\\partial w}$ et $\\frac{\\partial y}{\\partial b}$ en plus de $\\frac{\\partial y}{\\partial x}$, il suffit d'utiliser l'option `requires_grad=True` sur $w$ et $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des scalaires\n",
    "\n",
    "# Création des tenseurs scalaires\n",
    "x = torch.tensor(4., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "print(x.grad) # None\n",
    "print(w.grad) # None\n",
    "print(b.grad) # None\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z = w*x\n",
    "y = z+b\n",
    "\n",
    "# Calcul des gradients (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage des gradients\n",
    "print(x.grad)    # x.grad = 2.\n",
    "print(w.grad)    # w.grad = 4. \n",
    "print(b.grad)    # b.grad = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce mécanisme d'autograd fonctionne de la même manière lorsque les feuilles sont des tableaux multimensionnels plutôt que des scalaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple d'utilisation de l'autograd avec des tenseurs 2D\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "b = torch.tensor([4., 5.], requires_grad=True) #vecteur de taille 2\n",
    "\n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "z1 = X.matmul(W)\n",
    "z2 = z1 + b\n",
    "y = z2.sum()\n",
    "\n",
    "# Calcul des gradients (rétropropagation)\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "# Affichage des gradients\n",
    "print(X.grad)\n",
    "print(W.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessiner (sur papier) le graphe de calcul de l'exemple précédent. Quelle devrait-être les tailles des variables `X.grad`, `W.grad` et `b.grad` ? Vérifier leurs tailles dans le code (attribut `.shape` d'un tenseur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme nous venons de la voir, la fonctionnalité *autograd* est une implémentation du théorème de dérivation d'une fonction composée. Pour s'en convaincre, prenons le cas de la composition de deux fonctions $y=g(Z)$ et $Z=f(X)$. Nous allons comparer deux utilisations de l'autograd :\n",
    "\n",
    "Cas 1) Calculer directement $\\frac{\\partial y}{\\partial X}$ avec l'autograd\n",
    "\n",
    "Cas 2) Calculer manuellement $\\frac{\\partial y}{\\partial Z}$ (variable `dy_dZ`) et fournir ce gradient à l'autograd (`Z.backward(dy_dZ)`) pour qu'il termine le calcule de $\\frac{\\partial y}{\\partial X}$.\n",
    "\n",
    "Les gradients obtenus avec le cas 1 et le cas 2 doivent être parfaitement identiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAS 1\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "Z = X.matmul(W)\n",
    "y = Z.sum()\n",
    "\n",
    "y.backward(torch.tensor(1.))\n",
    "\n",
    "print(X.grad)\n",
    "# CAS 2\n",
    "\n",
    "# Création des tenseurs\n",
    "X = torch.tensor([[1., 2., 3.],[4., 5. ,6.], [7., 8. ,9.], [10., 11., 12.]],requires_grad=True) #tableau 4x3 \n",
    "W = torch.tensor([[1., 2.],[4., 5.], [7., 8.]],requires_grad=True) #tableau 3x2 \n",
    "\n",
    "# Construction du graphe de calcul (propagation avant)\n",
    "Z = X.matmul(W)\n",
    "y = Z.sum()\n",
    "\n",
    "dy_dZ = torch.ones(Z.shape) #dérivée de la fonction sum\n",
    "Z.backward(dy_dZ)\n",
    "\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Du MLP en Numpy au MLP en Pytorch sans *autograd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprendre le code Numpy du TP intitulé *TP MLP Numpy* et vérifier que le code fonctionne. L'objectif de cette partie est simplement de remplacer les `numpy.array` par des `torch.tensor` et de remplacer les appels aux fonctions Numpy par des appels aux fonctions équivalentes en PyTorch.\n",
    "\n",
    "Le code final ne doit plus faire aucun appel à la bibliothèque Numpy (il ne doit donc pas contenir la ligne `import numpy`).\n",
    "\n",
    "**Ne pas utiliser la fonctionnalité autograd de PyTorch, ni le paquet torch.nn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III) Utilisation de la fonctionnalité *autograd*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de cette partie consiste à remplacer dans le code de la partie précédente l'implémentation manuelle de la rétropropagation par la fonctionnalité *autograd*. Ainsi la méthode `def backward(self,dl_dO, O, X2, X1, X0)` de la classe `class MLP` doit être supprimée, et l'appel à cette méthode remplacé par l'appel à la méthode `.backward()` de l'autograd comme vu précédemment.\n",
    "\n",
    "Après avoir fini cette étape, ou si vous êtes bloqués, vous pourrez comparer votre implémentation au code ci-après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),torch.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = torch.tensor([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "y = torch.tensor([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)\n",
    "   \n",
    "class MLP:\n",
    "    def __init__(self, H, beta, lr):\n",
    "\n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        #parameters\n",
    "        self.W1 = ((2./self.D)*(2*(torch.rand(size=(self.D,self.H)))-0.5)).requires_grad_()\n",
    "        self.b1 = ((1./torch.sqrt(torch.tensor(self.D)))*(2*(torch.rand(size=(1,self.H)))-0.5)).requires_grad_()\n",
    "        self.W3 = ((2./self.H)*(2*(torch.rand(size=(self.H,self.C)))-0.5)).requires_grad_()\n",
    "        self.b3 = ((1./torch.sqrt(torch.tensor(self.H)))*(2*(torch.rand(size=(1,self.C)))-0.5)).requires_grad_()\n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = torch.zeros((self.D,self.H))\n",
    "        self.Vb1 = torch.zeros((self.H))\n",
    "        self.VW3 = torch.zeros((self.H,self.C))\n",
    "        self.Vb3 = torch.zeros((self.C))\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = X.matmul(self.W1) + self.b1 #NxH\n",
    "        X2 = torch.maximum(torch.tensor(0.),X1) #NxH\n",
    "        O = X2.matmul(self.W3) + self.b3 #NxC\n",
    "    \n",
    "        return O\n",
    "    \n",
    "        \n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.W1.grad.data\n",
    "            self.W1 -= self.lr*self.VW1\n",
    "    \n",
    "            self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.W3.grad.data\n",
    "            self.W3 -= self.lr*self.VW3\n",
    "        \n",
    "            self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.b1.grad.data\n",
    "            self.b1 -= self.lr*self.Vb1\n",
    "        \n",
    "            self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.b3.grad.data\n",
    "            self.b3 -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.W1.grad = None\n",
    "        self.b1.grad = None\n",
    "        self.W3.grad = None\n",
    "        self.b3.grad = None\n",
    "    \n",
    "  \n",
    "def logsoftmax(x):\n",
    "    x_shift = x - torch.amax(x, axis=1, keepdims=True)\n",
    "    return x_shift - torch.log(torch.exp(x_shift).sum(axis=1, keepdims=True))   \n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.amax(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "def multinoulliCrossEntropyLoss(O, y):\n",
    "    with torch.no_grad():\n",
    "        N = y.shape[0]\n",
    "        P = softmax(O.type(torch.float32))\n",
    "        log_p = logsoftmax(O.type(torch.float32))\n",
    "        a = log_p[torch.arange(N),y]\n",
    "        l = -a.sum()/N\n",
    "        dl_do = P\n",
    "        dl_do[torch.arange(N),y] -= 1\n",
    "        dl_do = dl_do/N\n",
    "    return (l, dl_do)\n",
    "        \n",
    "\n",
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    W: weight matrix\n",
    "    b: bias vector\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    O = model.forward(torch.hstack((torch.atleast_2d(xx.ravel()).T, torch.atleast_2d(yy.ravel()).T)))\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out\n",
    "\n",
    "#%% HYPERPARAMETERS\n",
    "H = 30\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations\n",
    "\n",
    "model = MLP(H,beta, lr)\n",
    "\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    [l, dl_dO] = multinoulliCrossEntropyLoss(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    acc = (torch.argmax(O, axis=1) == y).type(torch.float32).sum()/N\n",
    "    print('Iter {} | Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    model.zero_gradients()\n",
    "    O.backward(dl_dO)\n",
    "    \n",
    "    #Update Parameters\n",
    "    model.update()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if((i%10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Utilisation du paquet `torch.nn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En plus de la fonctionnalité autograd, la bibliothèque PyTorch contient de nombreuses implémentations de fonctions paramétriques qui permettent de construire une architecture beaucoup plus rapidement que ce que nous avons fait jusqu'à présent. Ces fonctions se trouvent dans le paquet `torch.nn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de la transformation affine générale (\"Fully Connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "x = torch.randn(10, 3)\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que cette fonctionnalité \"cache\" beaucoup de détails d'implémentation. Par exemple concernant la fonction `nn.linear`, ses paramètres sont définis implicitement ainsi que la méthode d'initialisation de leurs valeurs.\n",
    "\n",
    "En modifiant la classe `class MLP` (de l'implémentation utilisant l'autograd) en faisant usage du paquet `torch.nn` on obtient l'implémentation suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, H,  beta, lr):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.D, self.H) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.H, self.C)  \n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = torch.zeros((self.H,self.D))\n",
    "        self.Vb1 = torch.zeros((self.H))\n",
    "        self.VW3 = torch.zeros((self.C,self.H))\n",
    "        self.Vb3 = torch.zeros((self.C))\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = self.fc1(X) #NxH\n",
    "        X2 = self.relu(X1) #NxH\n",
    "        O = self.fc2(X2) #NxC\n",
    "    \n",
    "        return O\n",
    "    \n",
    "        \n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.fc1.weight.grad.data\n",
    "            self.fc1.weight -= self.lr*self.VW1\n",
    "    \n",
    "            self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.fc2.weight.grad.data\n",
    "            self.fc2.weight -= self.lr*self.VW3\n",
    "        \n",
    "            self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.fc1.bias.grad.data\n",
    "            self.fc1.bias -= self.lr*self.Vb1\n",
    "        \n",
    "            self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.fc2.bias.grad.data\n",
    "            self.fc2.bias -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.fc1.weight.grad = None\n",
    "        self.fc1.bias.grad = None\n",
    "        self.fc2.weight.grad = None\n",
    "        self.fc2.bias.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parmi les fonctions disponibles, `torch.nn` contient également les fonctions de coûts les plus communément utilisées. Ainsi la fonction `multinoulliCrossEntropyLoss` peut être remplacée par son équivalent PyTorch `nn.CrossEntropyLoss`. Ainsi le code complet prend la forme simplifiée suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),torch.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = torch.tensor([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "y = torch.tensor([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)\n",
    "   \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, H,  beta, lr):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        self.beta= beta\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.D, self.H) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.H, self.C)  \n",
    "        \n",
    "        #momentum\n",
    "        self.VW1 = torch.zeros((self.H,self.D))\n",
    "        self.Vb1 = torch.zeros((self.H))\n",
    "        self.VW3 = torch.zeros((self.C,self.H))\n",
    "        self.Vb3 = torch.zeros((self.C))\n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = self.fc1(X) #NxH\n",
    "        X2 = self.relu(X1) #NxH\n",
    "        O = self.fc2(X2) #NxC\n",
    "    \n",
    "        return O\n",
    "    \n",
    "        \n",
    "    def update(self):\n",
    "        with torch.no_grad():\n",
    "            self.VW1 = self.beta*self.VW1 + (1.0-self.beta)*self.fc1.weight.grad.data\n",
    "            self.fc1.weight -= self.lr*self.VW1\n",
    "    \n",
    "            self.VW3 = self.beta*self.VW3 + (1.0-self.beta)*self.fc2.weight.grad.data\n",
    "            self.fc2.weight -= self.lr*self.VW3\n",
    "        \n",
    "            self.Vb1 = self.beta*self.Vb1 + (1.0-self.beta)*self.fc1.bias.grad.data\n",
    "            self.fc1.bias -= self.lr*self.Vb1\n",
    "        \n",
    "            self.Vb3 = self.beta*self.Vb3 + (1.0-self.beta)*self.fc2.bias.grad.data\n",
    "            self.fc2.bias -= self.lr*self.Vb3\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        self.fc1.weight.grad = None\n",
    "        self.fc1.bias.grad = None\n",
    "        self.fc2.weight.grad = None\n",
    "        self.fc2.bias.grad = None\n",
    "    \n",
    "\n",
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    W: weight matrix\n",
    "    b: bias vector\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    O = model.forward(torch.hstack((torch.atleast_2d(xx.ravel()).T, torch.atleast_2d(yy.ravel()).T)))\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out\n",
    "\n",
    "#%% HYPERPARAMETERS\n",
    "H = 30\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations\n",
    "\n",
    "model = MLP(H,beta, lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    l = criterion(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    acc = (torch.argmax(O, axis=1) == y).type(torch.float32).sum()/N\n",
    "    print('Iter {} | Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    model.zero_gradients()\n",
    "    l.backward()\n",
    "    \n",
    "    #Update Parameters\n",
    "    model.update()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if((i%10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V) Utilisation du paquet `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs algorithmes d'optimisation sont également disponibles dans le paquet `torch.optim`. \n",
    "\n",
    "Lire la page de la documentation concernant ce paquet https://pytorch.org/docs/stable/optim.html?highlight=torch%20optim#module-torch.optim.\n",
    "\n",
    "Utiliser l'algorithme `torch.optim.SGD` pour simplifier le code précédent.\n",
    "\n",
    "Après avoir fini ce travail, vous pourrez comparer votre code au code suivant. Observer comme le code est beaucoup plus court par rapport au début du TP, mais un certain nombre de choses sont désormais cachées ou implicites (paramètres, initialisation des paramètres, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "#%% DEFINE AND PLOT DATA\n",
    "    \n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),torch.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "style_per_class = ['xb', 'or', 'sg']\n",
    "X = torch.tensor([[1.2, 2.3, -0.7, 3.2, -1.3],[-3.4, 2.8, 1.2, -0.4, -2.3]]).T\n",
    "y = torch.tensor([0,0,1,1,2])\n",
    "\n",
    "\n",
    "C = len(style_per_class)\n",
    "N = X.shape[0]\n",
    "xx, yy = make_meshgrid(X[:,0].ravel(), X[:,1].ravel(), h=0.1)\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "plt.grid(True)\n",
    "\n",
    "for i in range(C):\n",
    "    x_c = X[(y==i).ravel(),:]\n",
    "    plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "\n",
    "plt.pause(0.1)\n",
    "   \n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.C = 3\n",
    "        self.D = 2\n",
    "        self.H = H\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(self.D, self.H) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(self.H, self.C)  \n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "    \n",
    "        X1 = self.fc1(X) #NxH\n",
    "        X2 = self.relu(X1) #NxH\n",
    "        O = self.fc2(X2) #NxC\n",
    "    \n",
    "        return O\n",
    "    \n",
    "\n",
    "def plot_contours(ax, model, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    W: weight matrix\n",
    "    b: bias vector\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    O = model.forward(torch.hstack((torch.atleast_2d(xx.ravel()).T, torch.atleast_2d(yy.ravel()).T)))\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    Z = pred.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    \n",
    "    return out\n",
    "\n",
    "#%% HYPERPARAMETERS\n",
    "H = 30\n",
    "lr = 1e-2 #learning rate\n",
    "beta = 0.9 #momentum parameter\n",
    "n_epoch = 10000 #number of iterations\n",
    "\n",
    "model = MLP(H)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=beta)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    \n",
    "    #Forward Pass\n",
    "    O = model.forward(X)\n",
    "    \n",
    "    #Compute Loss\n",
    "    l = criterion(O, y)\n",
    "    \n",
    "    #Print Loss and Classif Accuracy\n",
    "    pred = torch.argmax(O, axis=1)\n",
    "    acc = (torch.argmax(O, axis=1) == y).type(torch.float32).sum()/N\n",
    "    print('Iter {} | Loss = {} | Training Accuracy = {}%'.format(i,l,acc*100))\n",
    "\n",
    "    #Backward Pass (Compute Gradient)\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    \n",
    "    #Update Parameters\n",
    "    optimizer.step()    \n",
    "    \n",
    "    \n",
    "    if((i%10)==0):\n",
    "        #Plot decision boundary\n",
    "        ax.cla()\n",
    "        for i in range(C):\n",
    "            x_c = X[(y==i).ravel(),:]\n",
    "            plt.plot(x_c[:,0],x_c[:,1],style_per_class[i])\n",
    "        plot_contours(ax, model, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        plt.pause(0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
