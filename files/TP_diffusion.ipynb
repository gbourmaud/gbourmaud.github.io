{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# TP diffusion (réseau débruiteur)\n",
    "Ce TP est une implémentation du modèle génératif profond [\"Denoising Diffusion Probabilistic Models\"](https://arxiv.org/abs/2006.11239) étudié en cours. Comme lors du TP VAE, le problème considéré est en 1D afin de pouvoir réaliser des affichages d'une part et de réduire le plus possible le temps d'entraînement d'autre part. **L'implémentation se fera en PyTorch.**\n",
    "1. Lancer une session linux (et non pas windows)\n",
    "2. Aller dans \"Applications\", puis \"Autre\", puis \"conda_pytorch\" (un terminal devrait s'ouvrir)\n",
    "3. Dans ce terminal, taper la commande suivante pour lancer Spyder : `spyder &`\n",
    "4. Configurer Spyder en suivant ces instructions : [Lien configuration Spyder](https://gbourmaud.github.io/files/configuration_spyder_annotated.pdf).\n",
    "5. Créer un dossier `TP_diffusion`.\n",
    "6. Créer un script python `tp.py` dans le dossier `TP_diffusion` et coller les lignes de code suivantes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Mélange de gaussiennes 1D\n",
    "Afin de s'assurer du bon fonctionnement de l'approche, nous allons considérer un cas d'école où $p_{data}(x)$ est **connue** et facile à échantillonner. Nous considérerons que $p_{data}(x)$ est un mélange de gaussiennes, ainsi la diffusion aura également une forme analytique (car \"diffuser\" un mélange de gaussiennes revient à \"diffuser\" chaque gaussienne du mélange). **Rappelons que dans un cas réel, $p_{data}(x)$ n'est pas connue. Ici, le fait de connaître $p_{data}(x)$ nous permet de faire de faire plus d'affichages pour mieux comprendre ce qu'il se passe.**  \n",
    "  \n",
    "Copier/coller les trois fonctions suivantes permettant d'afficher $p_{data}(x)$ et sa diffusion, d'afficher une simple gaussienne centrée réduite (ce qui nous permettra de vérifier visuellement qu'à la fin de la diffusion la distribution est très proche d'une gaussienne centrée réduite), et d'échantillonner $p_{data}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdata_diffusion(w_list, mu_list, sigma_list, alpha_diff, ax=None, orientation='vertical'):\n",
    "    \n",
    "    x = torch.linspace(start=-6., end=8., steps=1000)\n",
    "    \n",
    "    p_data = torch.zeros_like(x)\n",
    "    N_comp = w_list.shape[0]\n",
    "    for i in range(N_comp):\n",
    "        std_diff = math.sqrt(alpha_diff*(sigma_list[i]**2)+(1.-alpha_diff))\n",
    "        p_data_i = (1./(std_diff*math.sqrt(2*math.pi))*torch.exp(-0.5*((x-np.sqrt(alpha_diff)*mu_list[i])/std_diff)**2))\n",
    "        p_data += w_list[i]*p_data_i\n",
    "        \n",
    "    if(ax==None):\n",
    "        if(orientation=='vertical'):\n",
    "            plt.plot(x,p_data,'k')\n",
    "        else:\n",
    "            plt.plot(p_data,x,'k')\n",
    "    else:\n",
    "        if(orientation=='vertical'):\n",
    "            ax.plot(x,p_data,'k')\n",
    "        else:\n",
    "            ax.plot(p_data,x,'k')\n",
    "    return\n",
    "\n",
    "def plot_gaussian(ax=None, orientation='vertical'):\n",
    "    \n",
    "    x = torch.linspace(start=-6., end=8., steps=1000)\n",
    "        \n",
    "    p_data = (1./(math.sqrt(2*math.pi))*torch.exp(-0.5*(x)**2))\n",
    "        \n",
    "    if(ax==None):\n",
    "        if(orientation=='vertical'):\n",
    "            plt.plot(x,p_data,'r-')\n",
    "        else:\n",
    "            plt.plot(p_data,x,'r-')\n",
    "    else:\n",
    "        if(orientation=='vertical'):\n",
    "            ax.plot(x,p_data,'r-')\n",
    "        else:\n",
    "            ax.plot(p_data,x,'r-')\n",
    "    return\n",
    "\n",
    "def sample_from_pdata(N, w_list, mu_list, sigma_list):\n",
    "    \n",
    "    n_c = w_list.shape[0]\n",
    "\n",
    "    samp = torch.zeros((N,1))\n",
    "    mask = torch.multinomial(w_list,num_samples=N,replacement=True)\n",
    "    \n",
    "    for i in range(n_c):\n",
    "        samp_i = torch.normal(mean=mu_list[i], std=sigma_list[i], size=(N,1))\n",
    "        samp[mask==i] = samp_i[mask==i]\n",
    " \n",
    "    return samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir les paramètres du mélange de gaussiennes et afficher $p_{data}(x)$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "w_list = torch.tensor([0.2, 0.45, 0.35, 0.1, 0.3]) #poids du mélange de gaussiennes\n",
    "w_list /=w_list.sum() \n",
    "\n",
    "mu_list = torch.tensor([-3., 2.5, 1.5, -2.5, 5.]) #moyenne de chaque composante\n",
    "sigma_list = torch.tensor([0.3, 1.2, 0.3, 0.2, 0.1]) #écart-type de chaque composante\n",
    "\n",
    "plt.figure(1)\n",
    "plot_pdata_diffusion(w_list, mu_list, sigma_list, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais générer notre base de données en tirant des échantillons selon $p_{data}(x)$ (**RAPPEL : Dans un cas réel, ces échantillons sont donnés et $p_{data}(x)$ est inconnue. L'objectif d'un modèle génératif est justement d'apprendre à générer de nouveaux échantillons !**) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samp = int(2e4)\n",
    "X = sample_from_pdata(N_samp, w_list, mu_list, sigma_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définir le nombre d'étapes de diffusion $T$, les paramètres de diffusion $\\beta_t$ et calculer les paramètres $\\alpha_t$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50 #nombre de pas de diffusion\n",
    "beta_list_temp = np.linspace(1e-2,2e-1,T-1)\n",
    "beta_list = np.zeros(T)\n",
    "beta_list[1:] = beta_list_temp #ajout de beta_0 = 1 par convention\n",
    "alpha_list = np.cumprod((1.-beta_list)) #alpha_0 = 1 par convention\n",
    "print(alpha_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question : Les valeurs des $\\alpha_t$ sont affichées dans la console. À partir de ces valeurs, pensez-vous qu'à l'issue de la 50ème étape de diffusion les échantillons obtenus ressemblent aux échantillons d'une gaussienne centrée réduite ? Pourquoi ? (indice : regarder la dernière valeur de `alpha_list`)**  \n",
    "\n",
    "Confirmez visuellement votre réponse en affichant les distributions analytiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gen, axs_gen = plt.subplots(T//5,5)\n",
    "for t in range(T):\n",
    "    plot_pdata_diffusion(w_list, mu_list, sigma_list, alpha_list[t], ax = axs_gen[t//5,t%5])\n",
    "    plot_gaussian(ax = axs_gen[t//5,t%5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer la diffusion aux éléments de la base de données, et calculer les histogrammes pour vérifier que tout est cohérent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "#n, bins, patches = axs_gen[0,0].hist(X.numpy(), nbins, density=True, facecolor='g', alpha=0.75)\n",
    "plt.pause(0.1)\n",
    "Z_prec = X.clone()\n",
    "for t in range(T):\n",
    "    #Z_i = math.sqrt(1.-beta_list[i])*Z_prec + math.sqrt(beta_list[i])*t.normal(mean=0., std=1., size=(N_samp,1))\n",
    "    Z_t = math.sqrt(alpha_list[t])*X + math.sqrt(1-alpha_list[t])*torch.normal(mean=0., std=1., size=(N_samp,1))\n",
    "    Z_prec = Z_t.clone()\n",
    "    n, bins, patches = axs_gen[t//5,t%5].hist(Z_t.numpy(), nbins, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "plt.pause(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau débruiteur\n",
    "**Si besoin, commenter les affichages précédents pour accélérer l'éxecution du code.**  \n",
    "  \n",
    "Il faut désormais choisir une architecture de réseau de neurones $\\mu_\\theta (Z_t, t\\rightarrow t-1)$ permettant de débruiter $Z_t$ depuis l'instant $t$ vers l'instant $t-1$, ce qui est équivalent (avec la paramétrisation de la moyenne vue en cours) à considérer un réseau prédisant le bruit ayant été ajouté $\\hat\\epsilon_\\theta (Z_t, t\\rightarrow t-1)$. Par convention, en cours, nous avons remplacé la notation $t\\rightarrow t-1$ par le scalaire $t-1$. C'est ce scalaire qui est passé en entrée du réseau, et qui indique au réseau le traitement à effecteur (**il ne s'agit que d'une convention, on peut tout aussi bien choisir de remplacer $t\\rightarrow t-1$ par le scalaire $t$ plutôt que $t-1$**).\n",
    "  \n",
    "**À coder : Implémenter un perceptron multicouche avec deux couches cachées (FC->tanh->FC->tanh->FC). Les deux entrées scalaires $Z_t$ et $t-1$ seront transformées en un vecteur de dimension 3 : $[Z_t, cos(\\frac{t-1}{T}), sin(\\frac{t-1}{T})]$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage des paramètres du réseau débruiteur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À coder  : Implémenter la technique d'apprentissage du réseau débruiteur vue en cours, consistant à générer des minibatches, où pour chaque élément $X_i$ du minibatch, un temps de diffusion $t_i$ est tiré aléatoirement entre $1$ et $T$, conduisant à la fonction de coût suivante :**  \n",
    "**$\\sum_{i=1}^{\\text{batch size}}(\\epsilon_i - \\hat\\epsilon_\\theta (\\sqrt{\\alpha_{t_i}}X_i+\\sqrt{1-\\alpha_{t_i}}\\epsilon_i, t_i-1))^2$ avec $\\epsilon_i\\sim\\mathcal{N}(0,1)$.**  \n",
    "\n",
    "**Réaliser des affichages (coût, sortie du réseau à chaque pas de temps, génération de nouveaux échantillons,...) pour s'assurer que votre implémentation fonctionne correctement. Remarque : L'implémentation de ces affichages devrait prendre un certain du temps, en particulier la partie génération de nouveaux échantillons.**\n",
    "\n",
    "Une correction vous est proposée ci-après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def plot_pdata_diffusion(w_list, mu_list, sigma_list, alpha_diff, ax=None, orientation='vertical'):\n",
    "    \n",
    "    x = torch.linspace(start=-6., end=8., steps=1000)\n",
    "    \n",
    "    p_data = torch.zeros_like(x)\n",
    "    N_comp = w_list.shape[0]\n",
    "    for i in range(N_comp):\n",
    "        std_diff = math.sqrt(alpha_diff*(sigma_list[i]**2)+(1.-alpha_diff))\n",
    "        p_data_i = (1./(std_diff*math.sqrt(2*math.pi))*torch.exp(-0.5*((x-np.sqrt(alpha_diff)*mu_list[i])/std_diff)**2))\n",
    "        p_data += w_list[i]*p_data_i\n",
    "        \n",
    "    if(ax==None):\n",
    "        if(orientation=='vertical'):\n",
    "            plt.plot(x,p_data,'k')\n",
    "        else:\n",
    "            plt.plot(p_data,x,'k')\n",
    "    else:\n",
    "        if(orientation=='vertical'):\n",
    "            ax.plot(x,p_data,'k')\n",
    "        else:\n",
    "            ax.plot(p_data,x,'k')\n",
    "    return\n",
    "\n",
    "def plot_gaussian(ax=None, orientation='vertical'):\n",
    "    \n",
    "    x = torch.linspace(start=-6., end=8., steps=1000)\n",
    "        \n",
    "    p_data = (1./(math.sqrt(2*math.pi))*torch.exp(-0.5*(x)**2))\n",
    "        \n",
    "    if(ax==None):\n",
    "        if(orientation=='vertical'):\n",
    "            plt.plot(x,p_data,'r-')\n",
    "        else:\n",
    "            plt.plot(p_data,x,'r-')\n",
    "    else:\n",
    "        if(orientation=='vertical'):\n",
    "            ax.plot(x,p_data,'r-')\n",
    "        else:\n",
    "            ax.plot(p_data,x,'r-')\n",
    "    return\n",
    "\n",
    "def sample_from_pdata(N, w_list, mu_list, sigma_list):\n",
    "    \n",
    "    n_c = w_list.shape[0]\n",
    "\n",
    "    samp = torch.zeros((N,1))\n",
    "    mask = torch.multinomial(w_list,num_samples=N,replacement=True)\n",
    "    \n",
    "    for i in range(n_c):\n",
    "        samp_i = torch.normal(mean=mu_list[i], std=sigma_list[i], size=(N,1))\n",
    "        samp[mask==i] = samp_i[mask==i]\n",
    " \n",
    "    return samp\n",
    "\n",
    "\n",
    "\n",
    "w_list = torch.tensor([0.2, 0.45, 0.35, 0.1, 0.3])\n",
    "w_list /=w_list.sum() \n",
    "\n",
    "mu_list = torch.tensor([-3., 2.5, 1.5, -2.5, 5.])\n",
    "sigma_list = torch.tensor([0.3, 1.2, 0.3, 0.2, 0.1])\n",
    "\n",
    "T = 50\n",
    "beta_list_temp = np.linspace(1e-2,2e-1,T-1)\n",
    "#beta_list_temp = np.array([5e-2, 5e-2, 5e-2, 5e-2, 1e-1, 5e-1, 5e-1, 5e-1, 8e-1])#np.linspace(1e-2,1e-1,T)#np.array([5e-2, 5e-2, 5e-2, 5e-2, 1e-1, 5e-1, 5e-1, 5e-1, 5e-1])#np.linspace(5e-3,2e-1,9)\n",
    "beta_list = np.zeros(T)\n",
    "beta_list[1:] = beta_list_temp #ajout de beta_0 = 1 par convention\n",
    "print(beta_list)\n",
    "alpha_list = np.cumprod((1.-beta_list)) #alpha_0 = 1 par convention\n",
    "print(alpha_list)\n",
    "\n",
    "N_samp = int(2e4)\n",
    "X = sample_from_pdata(N_samp, w_list, mu_list, sigma_list)\n",
    "\n",
    "#sys.exit()    \n",
    "class denoiser(nn.Module):\n",
    "    def __init__(self,H):\n",
    "        super(denoiser, self).__init__()\n",
    "        \n",
    "        self.H = H\n",
    "        \n",
    "        self.linearIn = nn.Linear(3, H)\n",
    "        self.activIn = nn.Tanh()\n",
    "        \n",
    "        self.linearHidden = nn.Linear(H, H)\n",
    "        self.activHidden = nn.Tanh()\n",
    "        \n",
    "        self.linearOut = nn.Linear(H, 1)\n",
    " \n",
    "    def forward(self, z, t):\n",
    "\n",
    "        x = torch.cat((z, torch.cos(t), torch.sin(t)), dim=1)\n",
    "        out = self.linearIn(x)\n",
    "        out = self.activIn(out)\n",
    "        \n",
    "        out = self.linearHidden(out)\n",
    "        out = self.activHidden(out)\n",
    "        \n",
    "        noise = self.linearOut(out)\n",
    "\n",
    "        return noise\n",
    "    \n",
    "T = len(beta_list)\n",
    "H = 300\n",
    "learning_rate = 1e-3\n",
    "batchSize = 2048#256\n",
    "\n",
    "den = denoiser(H)\n",
    "optimizer = torch.optim.Adam(den.parameters(), lr=learning_rate)    \n",
    "NItMax = 5000\n",
    "alpha_list_t = torch.tensor(alpha_list).float()\n",
    "\n",
    "fig_map, axs_map = plt.subplots(T//5,5)\n",
    "fig_est, axs_est = plt.subplots(T//5,5)\n",
    "fig_curves, axs_curves = plt.subplots(1,1)\n",
    "loss_v = np.nan*np.zeros(int(NItMax/100))\n",
    "line_loss, = axs_curves.plot(np.linspace(0,NItMax,int(NItMax/100)),loss_v)\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i in range(NItMax):\n",
    "\n",
    "    if(i==4000):\n",
    "        optimizer.param_groups[0]['lr'] /= 10.\n",
    "    # if(i>10000):\n",
    "    #     for g in optimizer.param_groups:\n",
    "    #         g['lr'] = 1e-4\n",
    "\n",
    "    \n",
    "    perm = torch.randperm(N_samp)\n",
    "    X_batch = X[perm[:batchSize],:].float()\n",
    "    #t_batch = (T-1)*torch.ones((batchSize,1)).long()#\n",
    "    t_batch = torch.randint(low=1, high=T, size=(batchSize,1))\n",
    "    eps_batch = torch.normal(mean=0., std=1., size=(batchSize,1)).float()       \n",
    "    Z_batch = torch.sqrt(alpha_list_t[t_batch])*X_batch + torch.sqrt(1-alpha_list_t[t_batch])*eps_batch\n",
    "    t_batch_norm = (t_batch/T).float()\n",
    "    \n",
    "    eps_est = den(Z_batch,t_batch_norm)\n",
    "    \n",
    "    l = (((eps_est - eps_batch)**2).sum())/batchSize\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('It {} : loss : {:.2e}, lr : {}'.format(i, l.item(), optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    if(i%100 == 0):\n",
    "        loss_v[int(i/100)] = l.data\n",
    "        plt.figure(fig_curves.number)\n",
    "        #line_loss.set_ydata(loss_v)\n",
    "        #fig_curves.canvas.draw()\n",
    "        axs_curves.clear()\n",
    "        axs_curves.grid('on')\n",
    "        line_loss, = axs_curves.plot(np.linspace(0,NItMax,int(NItMax/100)),loss_v)\n",
    "        \n",
    "        plt.pause(0.1)\n",
    "    \n",
    "\n",
    "    #if((i<1000 and i%200==0) or i%2000 == 0):\n",
    "    if(i==NItMax-1):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            plt.figure(fig_map.number)\n",
    "            fig_map.suptitle('Iter {}'.format(i), fontsize=16)\n",
    "            for t in reversed(range(1,T)):\n",
    "\n",
    "                perm = torch.randperm(N_samp)\n",
    "                X_batch = X[perm[:batchSize],:].float()\n",
    "                t_batch = t*torch.ones((batchSize,1)).long()#\n",
    "                eps_batch = torch.normal(mean=0., std=1., size=(batchSize,1)).float()       \n",
    "                Z_batch = torch.sqrt(alpha_list_t[t_batch])*X_batch + torch.sqrt(1-alpha_list_t[t_batch])*eps_batch\n",
    "                t_batch_norm = (t_batch/T).float()\n",
    "                eps_est = den(Z_batch,t_batch_norm)\n",
    "    \n",
    "                Z_sort,ind = torch.sort(Z_batch[::10],dim=0)\n",
    "                eps_batch_temp = eps_batch[::10]\n",
    "                eps_batch_sort = eps_batch_temp[ind.view(-1)]\n",
    "                eps_est_temp = eps_est[::10]\n",
    "                eps_est_sort = eps_est_temp[ind.view(-1)]\n",
    "                axs_map[(t-1)//5,(t-1)%5].clear()\n",
    "                axs_map[(t-1)//5,(t-1)%5].plot(Z_sort, eps_batch_sort, label='a', color='b')\n",
    "                axs_map[(t-1)//5,(t-1)%5].plot(Z_sort, eps_est_sort, label='b', color='r', linestyle='dashed')\n",
    "                axs_map[(t-1)//5,(t-1)%5].grid('on')\n",
    "                #axs_map[(t-1)//5,(t-1)%5].legend()\n",
    "            \n",
    "            plt.pause(0.7)\n",
    "\n",
    "            plt.figure(fig_est.number)\n",
    "            fig_est.suptitle('Iter {}'.format(i), fontsize=16)\n",
    "            N_samp_val = 10000\n",
    "            Z_prec = torch.normal(mean=0., std=1., size=(N_samp_val,1)).float()\n",
    "            for t in reversed(range(1,T)):\n",
    "                #t = T-1\n",
    "                mu_t = (1./math.sqrt(1.-beta_list[t]))*(Z_prec - (beta_list[t]/math.sqrt(1.-alpha_list[t]))*den(Z_prec,(t/T)*torch.ones_like(Z_prec)))\n",
    "                sigma_t = math.sqrt(beta_list[t])\n",
    "                Z_t = mu_t + sigma_t*torch.normal(mean=0., std=1., size=(N_samp_val,1)).float()\n",
    "                Z_prec = Z_t.clone()\n",
    "                axs_est[(t-1)//5,(t-1)%5].clear()\n",
    "                plot_gaussian(ax = axs_est[(t-1)//5,(t-1)%5])\n",
    "                plot_pdata_diffusion(w_list, mu_list, sigma_list, alpha_list[t-1], ax = axs_est[(t-1)//5,(t-1)%5])\n",
    "                nbins = 100\n",
    "                n, bins, patches = axs_est[(t-1)//5,(t-1)%5].hist(Z_t.numpy(), nbins, density=True, facecolor='g', alpha=0.75)\n",
    "\n",
    "            plt.pause(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optionnel) MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'implémentation précedente concerne un exemple 1D. Modifier cette implémentation pour apprendre à générer de nouveaux échantillons de la base de données MNIST. Attention, une imagette de MNIST a 28x28 pixels, ainsi le temps d'apprentissage sera beaucoup plus long et nécessite une carte graphique (vous pouvez utiliser Google COLAB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
